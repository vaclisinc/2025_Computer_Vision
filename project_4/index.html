<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4: Neural Radiance Fields (NeRF)</title>
    <link rel="stylesheet" href="css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Space+Grotesk:wght@400;500;600;700&family=JetBrains+Mono&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <!-- Header -->
    <header>
        <div class="header-content">
            <div class="header-info">
                <a href="../index.html" class="back-link">← Back to Projects</a>
                <h1>Neural Radiance Fields (NeRF)</h1>
                <p class="subtitle">CS 180 Project 4 · Song-Ze Yu</p>
            </div>

            <div class="overview-grid">
                <div class="overview-card">
                    <h3>Part 0: Camera Calibration & 3D Scan</h3>
                    <p>Calibrate camera using ArUco markers, capture 3D object scan, estimate camera poses, and prepare dataset.</p>
                    <div class="part-nav-grid">
                        <a href="#part0-1" class="part-nav-item">
                            <span class="part-nav-number">0.1</span>
                            <span class="part-nav-title">Calibration</span>
                        </a>
                        <a href="#part0-2" class="part-nav-item">
                            <span class="part-nav-number">0.2</span>
                            <span class="part-nav-title">3D Scan</span>
                        </a>
                        <a href="#part0-3" class="part-nav-item">
                            <span class="part-nav-number">0.3</span>
                            <span class="part-nav-title">Pose</span>
                        </a>
                        <a href="#part0-4" class="part-nav-item">
                            <span class="part-nav-number">0.4</span>
                            <span class="part-nav-title">Dataset</span>
                        </a>
                    </div>
                </div>

                <div class="overview-card">
                    <h3>Part 1: Neural Field for 2D Images</h3>
                    <p>Train MLP with positional encoding to represent 2D images as continuous functions.</p>
                    <div class="part-nav-grid">
                        <a href="#part1-1" class="part-nav-item">
                            <span class="part-nav-number">1.1</span>
                            <span class="part-nav-title">Positional Encoding</span>
                        </a>
                        <a href="#part1-2" class="part-nav-item">
                            <span class="part-nav-number">1.2</span>
                            <span class="part-nav-title">MLP</span>
                        </a>
                        <a href="#part1-3" class="part-nav-item">
                            <span class="part-nav-number">1.3</span>
                            <span class="part-nav-title">Dataloader</span>
                        </a>
                        <a href="#part1-4" class="part-nav-item">
                            <span class="part-nav-number">1.4</span>
                            <span class="part-nav-title">Training</span>
                        </a>
                        <a href="#part1-5" class="part-nav-item">
                            <span class="part-nav-number">1.5</span>
                            <span class="part-nav-title">Results</span>
                        </a>
                    </div>
                </div>

                <div class="overview-card">
                    <h3>Part 2: Neural Radiance Field (3D)</h3>
                    <p>Generate rays, sample points, and render novel views through volume rendering.</p>
                    <div class="part-nav-grid">
                        <a href="#part2-1" class="part-nav-item">
                            <span class="part-nav-number">2.1</span>
                            <span class="part-nav-title">Rays</span>
                        </a>
                        <a href="#part2-2" class="part-nav-item">
                            <span class="part-nav-number">2.2</span>
                            <span class="part-nav-title">Sampling</span>
                        </a>
                        <a href="#part2-3" class="part-nav-item">
                            <span class="part-nav-number">2.3</span>
                            <span class="part-nav-title">Data</span>
                        </a>
                        <a href="#part2-4" class="part-nav-item">
                            <span class="part-nav-number">2.4</span>
                            <span class="part-nav-title">Network</span>
                        </a>
                        <a href="#part2-5" class="part-nav-item">
                            <span class="part-nav-number">2.5</span>
                            <span class="part-nav-title">Render</span>
                        </a>
                        <a href="#part2-6" class="part-nav-item">
                            <span class="part-nav-number">2.6</span>
                            <span class="part-nav-title">Custom</span>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Navigation -->
    <nav class="toc">
        <a href="#part0-1">0.1 Calibration</a>
        <a href="#part0-2">0.2 Photos</a>
        <a href="#part0-3">0.3 Pose</a>
        <a href="#part0-4">0.4 Dataset</a>
        <a href="#part1">1 2D NeRF</a>
        <a href="#part2-1">2.1 Rays</a>
        <a href="#part2-2">2.2 Sampling</a>
        <a href="#part2-3">2.3 Data</a>
        <a href="#part2-4">2.4 Network</a>
        <a href="#part2-5">2.5 Render</a>
        <a href="#part2-6">2.6 Custom</a>
    </nav>

    <p style="text-align: center; color: #666; font-size: 0.9rem; margin: 1rem 0;">
        Tip: every thumbnail on this page is clickable—tap to view the original resolution.
    </p>

    <main>
        <!-- PART 0: CAMERA CALIBRATION & 3D SCAN -->

        <section id="part0-1" class="info-section">
            <div class="content-centered">
                <h2>Part 0.1: Camera Calibration</h2>

                <p>The goal of camera calibration is to estimate the <strong>intrinsic matrix</strong> and <strong>distortion coefficients</strong> of the camera. These parameters are essential for later stages (pose estimation and 3D reconstruction) and must remain constant throughout the entire capture process — this is why the assignment spec emphasizes <strong>not adjusting the camera's focal length/zoom</strong>.</p>

                <p>By capturing multiple images of ArUco calibration tags from different angles and distances, we can solve for the camera's internal parameters using known 3D-to-2D correspondences.</p>

                <figure style="text-align: center; margin-top: 2rem;">
                    <img src="src/img/0-1-tags.jpg" alt="ArUco Tag Calibration" class="clickable" style="width: 70%;">
                    <figcaption>▲ Sample calibration images showing ArUco tag detection from various angles and distances (33 images total)</figcaption>
                </figure>
                <h3>Implementation</h3>

                <div class="observation-box">
                    <h4>Note: OpenCV Version Compatibility</h4>
                    <p>The spec uses syntax for OpenCV <strong>&lt; 4.7.0</strong>. My implementation uses the updated API for newer versions. See <a href="https://stackoverflow.com/questions/76186376/attributeerror-module-cv2-aruco-has-no-attribute-detectmarkers-python" target="_blank">this StackOverflow discussion</a> for details.</p>
                </div>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> Camera Calibration Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def calibrate_camera(img_dir, tag_size=0.06):
    # Updated API for OpenCV >= 4.7.0
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    obj_points = []  # 3D points in real world space
    img_points = []  # 2D points in image plane

    # Define 3D coordinates of ArUco tag corners
    tag_3d = np.array([
        [0, 0, 0],
        [tag_size, 0, 0],
        [tag_size, tag_size, 0],
        [0, tag_size, 0]
    ], dtype=np.float32)

    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))
    img_shape = None

    # Detect tags in all calibration images
    for img_path in img_files:
        img = cv2.imread(img_path)
        if img_shape is None:
            img_shape = img.shape[:2]

        corners, ids, _ = detector.detectMarkers(img)

        if ids is not None:
            for corner in corners:
                obj_points.append(tag_3d)
                img_points.append(corner.reshape(-1, 2))

    # Solve for camera intrinsics and distortion
    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(
        obj_points, img_points, img_shape[::-1], None, None
    )

    return mtx, dist, rvecs, tvecs</code></pre>
                    </div>
                </div>

                <div class="code-block">
                    <details>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▶</span>
                            <strong>Camera path + novel views</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">def create_circular_path(base_pos, num_frames):
    radius = np.sqrt(base_pos[0]**2 + base_pos[1]**2)
    fixed_z = base_pos[2]
    c2ws = []
    for i in range(num_frames):
        angle = (i / num_frames) * 2 * np.pi
        cam_x = radius * np.cos(angle)
        cam_y = radius * np.sin(angle)
        cam_pos = np.array([cam_x, cam_y, fixed_z])

        forward = -cam_pos / np.linalg.norm(cam_pos)
        world_up = np.array([0, 0, 1])
        right = np.cross(forward, world_up); right /= np.linalg.norm(right)
        up = np.cross(right, forward); up /= np.linalg.norm(up)

        c2w = np.eye(4)
        c2w[:3, 0] = right
        c2w[:3, 1] = up
        c2w[:3, 2] = forward
        c2w[:3, 3] = cam_pos
        c2ws.append(c2w)
    return c2ws

def render_novel_views(model, data_path, num_frames, save_path,
                       n_samples, near, far, device, white_bkgd=True):
    data = np.load(data_path)
    c2ws_train = data['c2ws_train']
    K = data['K'] if 'K' in data else np.array([
        [data['focal'], 0, data['images_train'].shape[2] / 2],
        [0, data['focal'], data['images_train'].shape[1] / 2],
        [0, 0, 1]
    ])
    H, W = data['images_train'].shape[1:3]

    c2ws = create_circular_path(c2ws_train[0, :3, 3], num_frames)
    eval_model = model.module if hasattr(model, 'module') else model
    eval_model.eval()

    frames = []
    with torch.no_grad():
        for c2w in c2ws:
            frame = render_image(eval_model,
                                 torch.FloatTensor(c2w),
                                 H, W, K,
                                 n_samples=n_samples,
                                 near=near, far=far,
                                 device=device)
            frames.append((np.clip(frame, 0, 1) * 255).astype(np.uint8))

    pil_frames = [Image.fromarray(f) for f in frames]
    pil_frames[0].save(save_path, save_all=True,
                       append_images=pil_frames[1:],
                       duration=33, loop=0)</code></pre>
                        </div>
                    </details>
                </div>

                <h3>Results</h3>

                <div class="result-box">
                    <p><strong>Images processed:</strong> 33/33 images (100% detection rate)</p>
                    <p><strong>Total tag detections:</strong> 198 tags</p>
                    <p><strong>Reprojection Error:</strong> 0.786 pixels</p>

                    <p style="margin-top: 1.5rem;"><strong>Camera Intrinsic Matrix (K):</strong></p>
                    <pre style="background: white; padding: 0.75rem; border-radius: 4px; margin: 0.5rem 0; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem;">
[[957.34    0.00  795.40]
 [  0.00  949.21  594.72]
 [  0.00    0.00    1.00]]</pre>

                    <p style="margin-top: 1rem;"><strong>Distortion Coefficients:</strong> [k₁, k₂, p₁, p₂, k₃]</p>
                    <pre style="background: white; padding: 0.75rem; border-radius: 4px; margin: 0.5rem 0; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem;">
[0.261, -1.723, 0.018, 0.027, 2.748]</pre>
                </div>


            </div>
        </section>

        <section id="part0-2" class="info-section">
            <div class="content-centered">
                <h2>Part 0.2: Capturing a 3D Object Scan</h2>

                <p>I placed my <strong>Miffy</strong> doll next to the calibration tag and shot <strong>38 photos</strong> while orbiting the bunny, keeping exposure/focus fixed so the poses stay consistent. The raw captures live at <code class="code-inline">images/0-miffy-two/</code>. (Fun fact: I first tried the CalHacks 12.0 bear, but the busy background made training unstable, so I re-shot with Miffy—Part&nbsp;1 still uses the bear!)</p>

                <figure style="text-align: center;">
                    <img src="src/img/0-2-miffy-calbear.jpg" alt="Miffy and CalBear comparison" class="clickable" style="width: 70%;">
                    <figcaption>▲ Left: new Miffy capture used for Part&nbsp;2.     ▲ Right: original CalHacks bear from Part&nbsp;1.</figcaption>
                </figure>

                <p>Before training Part&nbsp;2, I resize the photos down to <code class="code-inline">373×280</code> so they match the intrinsics of the provided Lego dataset. A tiny Pillow script batches the conversion:</p>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Batch resizing script</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">import os
from PIL import Image

input_folder = "images/0-miffy-two"
output_folder = "images/0-miffy-two-resize"
target_size = (373, 280)

os.makedirs(output_folder, exist_ok=True)

for filename in os.listdir(input_folder):
    if filename.lower().endswith((".jpg", ".jpeg", ".png")):
        img_path = os.path.join(input_folder, filename)
        img = Image.open(img_path)
        img_resized = img.resize(target_size, Image.LANCZOS)
        img_resized.save(os.path.join(output_folder, filename))
        print(f"Done: {filename}")

print("All images resized!")</code></pre>
                        </div>
                    </details>
                </div>

                <p>The resized set in <code class="code-inline">images/0-miffy-two-resize/</code> becomes the dataset I feed into the NeRF dataloader later on.</p>

            </div>
        </section>

        <section id="part0-3" class="info-section">
            <div class="content-centered">
                <h2>Part 0.3: Camera Pose Estimation</h2>

                <p>The goal of this step is to estimate the <strong>extrinsic parameters</strong> (rotation and translation) for each camera pose. This is known as the <strong>Perspective-n-Point (PnP)</strong> problem: given a set of 3D points in world coordinates and their corresponding 2D projections in an image, we solve for the camera's position and orientation in 3D space.</p>

                <h3>Implementation</h3>

                <div class="observation-box">
                    <h4>Important: World-to-Camera vs Camera-to-World</h4>
                    <For>OpenCV's <code class="code-inline">solvePnP()</code> returns the <strong>world-to-camera (w2c)</strong> transformation.<br>For NeRF, we need the <strong>camera-to-world (c2w)</strong> transformation, so we must invert the result:</p>
                    <pre style="background: white; padding: 0.75rem; border-radius: 4px; margin: 0.5rem 0; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem; line-height: 1.8;">
c2w = [R | t] = [R₀₀  R₀₁  R₀₂  tₓ]
      [0 | 1]   [R₁₀  R₁₁  R₁₂  tᵧ]
                [R₂₀  R₂₁  R₂₂  t_z]
                [ 0    0    0    1 ]</pre>
                </div>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> Pose Estimation Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def estimate_camera_poses(img_dir, mtx, dist, tag_size=0.02):
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    # 3D coordinates of single ArUco tag
    tag_3d = np.array([
        [0, 0, 0],
        [tag_size, 0, 0],
        [tag_size, tag_size, 0],
        [0, tag_size, 0]
    ], dtype=np.float32)

    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))
    poses = []
    images = []

    for i, img_path in enumerate(img_files):
        img = cv2.imread(img_path)
        if img is None:
            continue

        corners, ids, _ = detector.detectMarkers(img)

        if ids is not None and len(ids) > 0:
            img_points = corners[0].reshape(-1, 2)

            # Solve PnP for world-to-camera transformation
            success, rvec, tvec = cv2.solvePnP(
                tag_3d, img_points, mtx, dist
            )

            if success:
                R, _ = cv2.Rodrigues(rvec)

                # Build w2c matrix
                w2c = np.eye(4)
                w2c[:3, :3] = R
                w2c[:3, 3] = tvec.flatten()

                # Invert to get c2w for NeRF
                c2w = np.linalg.inv(w2c)

                poses.append({
                    'idx': i,
                    'filename': os.path.basename(img_path),
                    'c2w': c2w
                })
                images.append(img)

    return poses, images</code></pre>
                    </div>
                </div>

                <h3>Visualization with Viser</h3>

                <p>To verify our pose estimation, we visualize all camera frustums in 3D space using <code class="code-inline">viser</code>.</p>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> 3D Frustum Visualization
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def visualize_camera_poses(poses, images, mtx):
    server = viser.ViserServer(share=True)

    H, W = images[0].shape[:2]

    for i, (pose_data, img) in enumerate(zip(poses, images)):
        c2w = pose_data['c2w']
        server.scene.add_camera_frustum(
            f"/cameras/{i}",
            fov=2 * np.arctan2(H / 2, mtx[0, 0]),
            aspect=W / H,
            scale=0.02,
            wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz,
            position=c2w[:3, 3],
            image=img
        )

    try:
        while True:
            time.sleep(0.1)</code></pre>
                    </div>
                </div>

                <h3>Results</h3>

                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 1.5rem 0;">
                    <figure>
                        <img src="src/img/0-3-viser-demo1.png" alt="Camera Frustums View 1" class="clickable" style="height: 300px; object-fit: cover;">
                        <figcaption>▲ 3D visualization of camera frustums (view 1)</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/0-3-viser-demo2.png" alt="Camera Frustums View 2" class="clickable" style="height: 300px; object-fit: cover;">
                        <figcaption>▲ 3D visualization of camera frustums (view 2)</figcaption>
                    </figure>
                </div>
            </div>
        </section>

        <section id="part0-4" class="info-section">
            <div class="content-centered">
                <h2>Part 0.4: Dataset Preparation</h2>

                <p>The final step is to <strong>undistort all images</strong> and package everything into a dataset format ready for NeRF training. This combines all previous steps from Part 0.1 to 0.3.</p>

                <h3>Implementation</h3>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> Dataset Creation Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def create_nerf_dataset(img_dir, mtx, dist, tag_size=0.06,
                        train_ratio=0.7, val_ratio=0.15,
                        output_path='output/my_data.npz'):
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    tag_3d = np.array([
        [0, 0, 0],
        [tag_size, 0, 0],
        [tag_size, tag_size, 0],
        [0, tag_size, 0]
    ], dtype=np.float32)

    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))
    all_images = []
    all_c2ws = []

    for img_path in img_files:
        img = cv2.imread(img_path)
        if img is None:
            continue

        corners, ids, _ = detector.detectMarkers(img)

        if ids is not None and len(ids) > 0:
            img_points = corners[0].reshape(-1, 2)
            success, rvec, tvec = cv2.solvePnP(tag_3d, img_points, mtx, dist)

            if success:
                # Key steps: undistort and convert color
                undistorted = cv2.undistort(img, mtx, dist)
                undistorted_rgb = cv2.cvtColor(undistorted, cv2.COLOR_BGR2RGB)

                # Compute c2w matrix
                R, _ = cv2.Rodrigues(rvec)
                w2c = np.eye(4)
                w2c[:3, :3] = R
                w2c[:3, 3] = tvec.flatten()
                c2w = np.linalg.inv(w2c)

                all_images.append(undistorted_rgb)
                all_c2ws.append(c2w)

    all_images = np.array(all_images)
    all_c2ws = np.array(all_c2ws)

    # Split into train/val/test
    n_total = len(all_images)
    n_train = int(n_total * train_ratio)
    n_val = int(n_total * val_ratio)

    idx = np.random.permutation(n_total)
    train_idx = idx[:n_train]
    val_idx = idx[n_train:n_train + n_val]
    test_idx = idx[n_train + n_val:]

    focal = mtx[0, 0]

    # Save as .npz
    np.savez(
        output_path,
        images_train=all_images[train_idx],
        c2ws_train=all_c2ws[train_idx],
        images_val=all_images[val_idx],
        c2ws_val=all_c2ws[val_idx],
        c2ws_test=all_c2ws[test_idx],
        # focal=focal
        K=mtx
    )

    return (all_images[train_idx], all_c2ws[train_idx],
            all_images[val_idx], all_c2ws[val_idx],
            all_c2ws[test_idx], focal)</code></pre>
                    </div>
                </div>
                <h3>Dataset Structure</h3>

                <div class="result-box">
                    <h4>Output: my_data.npz</h4>
                    <p>The packaged dataset contains:</p>
                    <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
                        <li><code class="code-inline">images_train</code>: Training images (undistorted RGB)</li>
                        <li><code class="code-inline">c2ws_train</code>: Camera-to-world matrices for training (0.7)</li>
                        <li><code class="code-inline">images_val</code>: Validation images</li>
                        <li><code class="code-inline">c2ws_val</code>: Camera-to-world matrices for validation (0.15)</li>
                        <li><code class="code-inline">c2ws_test</code>: Camera-to-world matrices for test (0.15)</li>
                        <li><code class="code-inline">K</code>: Full intrinsic matrix (switched from scalar focal because it made Part&nbsp;2.6 result better)</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- PART 1: NEURAL FIELD FOR 2D IMAGES -->

        <section id="part1" class="info-section">
            <div class="content-centered">
                <h2>Part 1: Neural Field for 2D Images</h2>

                <p>Before jumping into 3D NeRF, which learns the mapping <code class="code-inline">F: {x, y, z, θ, φ} → {r, g, b, σ}</code>, we start with 2D as a simpler introduction. In 2D, we only need to solve <code class="code-inline">F: {u, v} → {r, g, b}</code>, where <code class="code-inline">{u, v}</code> represents 2D pixel coordinates.</p>

                <!-- Positional Encoding -->
                <h3 id="part1-1">1.1 Sinusoidal Positional Encoding (PE)</h3>

                <br>Positional encoding helps neural networks capture <strong>high-frequency details</strong> by transforming input coordinates into a higher-dimensional space using sinusoidal functions. Without PE, MLPs struggle to represent fine details in images.</p>

                <p>The encoding applies sine and cosine functions at different frequencies:</p>
                <figure>
                    <img src="src/img/1-pe.jpg" alt="2D Neural Field Architecture" class="clickable">
                </figure>
                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Positional Encoding Implementation</strong>
                        </summary>
                        <div class="code-content">
                            <pre><code class="language-python">class PositionalEncoding(nn.Module):
    def __init__(self, L=10):
        super().__init__()
        self.L = L

    def forward(self, x):
        out = [x]
        for i in range(self.L):
            out.append(torch.sin(2**i * np.pi * x))
            out.append(torch.cos(2**i * np.pi * x))
        return torch.cat(out, dim=-1)</code></pre>
                        </div>
                    </details>
                </div>

                <p>For <code class="code-inline">L=10</code> frequency bands, a 2D coordinate is transformed from dimension 2 to dimension <code class="code-inline">2 + 2×L×2 = 42</code>.</p>

                <!-- MLP Architecture -->
                <h3 id="part1-2">1.2 Multilayer Perceptron (MLP) Architecture</h3>

                <p>Our neural field network follows the architecture shown below:</p>

                <figure>
                    <img src="src/img/1-mlp_img.jpg" alt="2D Neural Field Architecture" class="clickable">
                    <figcaption>▲ Network architecture: 2D coordinates → PE → 3 hidden layers (256 units) → RGB output</figcaption>
                </figure>

                <p>The network consists of:</p>
                <ul>
                    <li><strong>Input:</strong> 2D pixel coordinates (u, v)</li>
                    <li><strong>Positional Encoding:</strong> Expands input to 42 dimensions</li>
                    <li><strong>Hidden Layers:</strong> 3 fully-connected layers with 256 units each, using ReLU activation</li>
                    <li><strong>Output Layer:</strong> Maps to RGB values (3 channels) with Sigmoid activation to ensure [0,1] range</li>
                </ul>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Neural Field 2D Implementation</strong>
                        </summary>
                        <div class="code-content">
                            <pre><code class="language-python">class NeuralField2D(nn.Module):
    def __init__(self, L=10, hidden_dim=256):
        super().__init__()
        self.pe = PositionalEncoding(L)
        pe_dim = 2 + 2 * L * 2  # 42 dimensions

        self.mlp = nn.Sequential(
            nn.Linear(pe_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 3),
            nn.Sigmoid()
        )

    def forward(self, coords):
        encoded = self.pe(coords)
        return self.mlp(encoded)</code></pre>
                        </div>
                    </details>
                </div>

                <!-- Dataloader -->
                <h3 id="part1-3">1.3 Pixel Dataloader</h3>

                <p>Since high-resolution images are memory-intensive, we implement a dataloader that randomly samples <code class="code-inline">N</code> pixels at every iteration for training. This approach:</p>
                <ul>
                    <li>Returns <code class="code-inline">N×2</code> normalized 2D coordinates in [0,1]</li>
                    <li>Returns corresponding <code class="code-inline">N×3</code> RGB color values in [0,1]</li>
                    <li>Enables efficient mini-batch training without loading entire images</li>
                </ul>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Pixel Dataloader Implementation</strong>
                        </summary>
                        <div class="code-content">
                            <pre><code class="language-python">class PixelDataloader:
    def __init__(self, img, batch_size=10000):
        self.img = img / 255.0  # Normalize to [0,1]
        self.H, self.W = img.shape[:2]
        self.batch_size = batch_size

        # Create normalized coordinate grid
        y, x = np.meshgrid(np.arange(self.H), np.arange(self.W), indexing='ij')
        self.coords = np.stack([x / self.W, y / self.H], axis=-1).reshape(-1, 2)
        self.colors = self.img.reshape(-1, 3)

    def sample(self):
        """Randomly sample batch_size pixels"""
        idx = np.random.choice(len(self.coords), self.batch_size, replace=False)
        coords = torch.FloatTensor(self.coords[idx])
        colors = torch.FloatTensor(self.colors[idx])
        return coords, colors

    def get_all(self):
        """Get all pixels (for validation/visualization)"""
        coords = torch.FloatTensor(self.coords)
        colors = torch.FloatTensor(self.colors)
        return coords, colors</code></pre>
                        </div>
                    </details>
                </div>

                <!-- Training -->
                <h3 id="part1-4">1.4 Training: Loss Function, Optimizer, and Metric</h3>

                <p>We train the neural field to minimize the difference between predicted and ground truth pixel colors using:</p>
                <ul>
                    <li><strong>Loss Function:</strong> Mean Squared Error (MSE) between predicted and actual RGB values</li>
                    <li><strong>Metric:</strong> Peak Signal-to-Noise Ratio (PSNR) = <code class="code-inline">10 × log₁₀(1/MSE)</code></li>
                    <li><strong>Optimizer:</strong> Adam with learning rate <code class="code-inline">1e-2</code></li>
                    <li><strong>Training:</strong> 1000-3000 iterations with batch size of 10,000 pixels</li>
                </ul>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Training Loop Implementation</strong>
                        </summary>
                        <div class="code-content">
                            <pre><code class="language-python">def train_neural_field(img_path, L=10, hidden_dim=256, lr=1e-2, iters=3000, batch_size=10000, save_dir='images'):
    os.makedirs(save_dir, exist_ok=True)

    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
    print(f'Using device: {device}')

    img = skio.imread(img_path)
    if img.ndim == 2:
        img = np.stack([img] * 3, axis=-1)

    H, W = img.shape[:2]

    model = NeuralField2D(L=L, hidden_dim=hidden_dim).to(device)
    dataloader = PixelDataloader(img, N=batch_size)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    psnr_history = []
    save_iters = [0, 100, 500, 1000, 2000, iters-1]

    for it in range(iters):
        coords, colors = dataloader.sample()
        coords, colors = coords.to(device), colors.to(device)

        pred_colors = model(coords)
        loss = criterion(pred_colors, colors)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        psnr = psnr_from_mse(loss)
        psnr_history.append(psnr.item())

        if it % 100 == 0 or it == iters - 1:
            print(f'Iter {it:4d}: Loss={loss.item():.6f}, PSNR={psnr.item():.2f} dB')

        # Save visualization at key iterations
        if it in save_iters:
            with torch.no_grad():
                y, x = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')
                test_coords = np.stack([x / W, y / H], axis=-1).reshape(-1, 2)
                test_coords = torch.FloatTensor(test_coords).to(device)

                pred = []
                for i in range(0, len(test_coords), batch_size):
                    pred.append(model(test_coords[i:i+batch_size]).cpu())
                pred = torch.cat(pred, dim=0)

                pred_img = pred.reshape(H, W, 3).numpy()

                # Save comparison: GT, Prediction, Error
                plt.figure(figsize=(12, 4))
                plt.subplot(1, 3, 1)
                plt.imshow(img / 255.0)
                plt.title('Ground Truth')
                plt.axis('off')

                plt.subplot(1, 3, 2)
                plt.imshow(pred_img)
                plt.title(f'Prediction (iter {it})')
                plt.axis('off')

                plt.subplot(1, 3, 3)
                plt.imshow(np.abs(img / 255.0 - pred_img))
                plt.title('Error')
                plt.axis('off')

                plt.tight_layout()
                plt.savefig(f'{save_dir}/iter_{it:04d}.png', bbox_inches='tight', dpi=150)
                plt.close()

    # Save PSNR curve
    plt.figure(figsize=(10, 6))
    plt.plot(psnr_history)
    plt.xlabel('Iteration')
    plt.ylabel('PSNR (dB)')
    plt.title('Training PSNR')
    plt.grid(True)
    plt.savefig(f'{save_dir}/psnr_curve.png', bbox_inches='tight', dpi=150)
    plt.close()

    return model, psnr_history</code></pre>
                        </div>
                    </details>
                </div>

                <!-- Results -->
                <h3 id="part1-5">1.5 Results and Analysis</h3>

                <h4>1) Model Architecture</h4>
                <div class="result-box" style="padding: 1.25rem;">
                    <ul style="line-height: 1.6; margin: 0 0 0 1.25rem;">
                        <li><strong>Positional Encoding:</strong> L = 10 frequency bands (input dimension: 42)</li>
                        <li><strong>Network Depth:</strong> 3 hidden layers</li>
                        <li><strong>Layer Width:</strong> 256 units per hidden layer</li>
                        <li><strong>Activation:</strong> ReLU (hidden layers), Sigmoid (output layer)</li>
                        <li><strong>Learning Rate:</strong> 1e-2 (Adam optimizer)</li>
                        <li><strong>Batch Size:</strong> 10,000 pixels per iteration</li>
                        <li><strong>Training Iterations:</strong> 3,000</li>
                    </ul>
                </div>

                <h4>2) Training Progression</h4>
                <p>The following grids show how the neural field sharpens each image across training iterations.</p>

                <div style="margin-bottom: 1.5rem;">
                    <p style="font-weight: 600; margin-bottom: 0.5rem;">Test Image (Fox)</p>
                    <div style="display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 0.9rem;">
                        <figure style="margin: 0;">
                            <img src="src/img/1-fox/iter_0000.png" alt="Fox - Iteration 0" class="clickable">
                            <figcaption>▲ Iter 0</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-fox/iter_0100.png" alt="Fox - Iteration 100" class="clickable">
                            <figcaption>▲ Iter 100</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-fox/iter_0500.png" alt="Fox - Iteration 500" class="clickable">
                            <figcaption>▲ Iter 500</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-fox/iter_1000.png" alt="Fox - Iteration 1000" class="clickable">
                            <figcaption>▲ Iter 1000</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-fox/iter_2000.png" alt="Fox - Iteration 2000" class="clickable">
                            <figcaption>▲ Iter 2000</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-fox/iter_2999.png" alt="Fox - Iteration 2999" class="clickable">
                            <figcaption>▲ Iter 2999</figcaption>
                        </figure>
                    </div>
                </div>

                <div style="margin-bottom: 1.5rem;">
                    <p style="font-weight: 600; margin-bottom: 0.5rem;">My Own Image (CalHacks Bear)</p>
                    <div style="display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 0.9rem;">
                        <figure style="margin: 0;">
                            <img src="src/img/1-calbear/iter_0000.png" alt="CalBear - Iteration 0" class="clickable">
                            <figcaption>▲ Iter 0</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-calbear/iter_0100.png" alt="CalBear - Iteration 100" class="clickable">
                            <figcaption>▲ Iter 100</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-calbear/iter_0500.png" alt="CalBear - Iteration 500" class="clickable">
                            <figcaption>▲ Iter 500</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-calbear/iter_1000.png" alt="CalBear - Iteration 1000" class="clickable">
                            <figcaption>▲ Iter 1000</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-calbear/iter_2000.png" alt="CalBear - Iteration 2000" class="clickable">
                            <figcaption>▲ Iter 2000</figcaption>
                        </figure>
                        <figure style="margin: 0;">
                            <img src="src/img/1-calbear/iter_2999.png" alt="CalBear - Iteration 2999" class="clickable">
                            <figcaption>▲ Iter 2999</figcaption>
                        </figure>
                    </div>
                </div>

                <h4>3) Hyperparameter Tuning</h4>
                <p>We experiment with different values of L and layer width to understand their impact on reconstruction quality. <br>Using very low values reveals how these hyperparameters affect the model's ability to capture fine details.</p>

                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 0.5rem 0 1.5rem 0;">
                    <figure>
                        <img src="src/img/1-fox/hyperparam_comparison.png" style="height: 400px; object-fit: cover;"alt="Fox - Hyperparameter Grid" class="clickable" style="width: 100%;">
                        <figcaption>Fox: effect of L and layer width.</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-calbear/hyperparam_comparison.png" style="height: 400px; object-fit: cover;"alt="CalBear - Hyperparameter Grid" class="clickable" style="width: 100%;">
                        <figcaption>CalBear: same search space, similar trends.</figcaption>
                    </figure>
                </div>

                <div class="observation-box">
                    <h4>Observations</h4>
                    <ul>
                        <li><strong>Positional Encoding (L):</strong> Lower L values struggle to capture high-frequency details, resulting in blurry outputs. Higher L values enable the network to represent fine textures and edges.</li>
                        <li><strong>Layer Width:</strong> Narrower networks (fewer neurons per layer) have limited capacity, leading to smoother, less detailed reconstructions. Wider networks can fit more complex patterns.</li>
                    </ul>
                </div>

                <h4>4) Training PSNR Curves</h4>
                <p>This shows reconstruction quality improvement during training. Higher PSNR indicates better image quality.</p>

                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 1.5rem; margin: 1.5rem 0;">
                    <figure>
                        <img src="src/img/1-fox/psnr_curve.png" alt="Fox PSNR Curve" class="clickable">
                        <figcaption>▲ Fox training PSNR progression</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-calbear/psnr_curve.png" alt="CalBear PSNR Curve" class="clickable">
                        <figcaption>▲ CalHacks bear training PSNR progression</figcaption>
                    </figure>
                </div>

                <p>The <code class="code-inline">__main__</code> block above performs a quick sanity sweep: verifying UV indexing, sampling random rays, and ensuring <code class="code-inline">sample_along_rays</code> still produces reasonable shapes before plugging the dataloader into training.</p>

            </div>
        </section>

        <!-- PART 2: NEURAL RADIANCE FIELD (3D) -->

        <section id="part2-1" class="info-section">
            <div class="content-centered">
                <h2>Part 2.1: Ray Generation</h2>
                <p>This step turns every 2D pixel into a 3D ray—that’s the data the NeRF will actually learn from. For each pixel we recover its camera-space direction, rotate/translate it into world space using the pose, and treat the camera center plus that direction as a ray ready for sampling.</p>

                <div class="observation-box">
                    <h4>Coordinate Conventions</h4>
                    <ul>
                        <li><strong>Intrinsic matrix K:</strong> Encodes the focal lengths <code class="code-inline">f_x, f_y</code> and principal point <code class="code-inline">(o_x, o_y)</code>. I assume square pixels so <code class="code-inline">f_x = f_y = focal</code> when using the provided synthetic datasets.</li>
                        <li><strong>Extrinsics:</strong> I store poses as camera-to-world (c2w) matrices, which is the inverse of OpenCV’s w2c output. Each pose is <code class="code-inline">[R | t]</code> where <code class="code-inline">t</code> is the camera origin expressed in world space.</li>
                        <li><strong>Pixel centers:</strong> Following the spec, I add <code class="code-inline">0.5</code> to each integer pixel index so that rays pass through pixel centers when forming the meshgrid.</li>
                    </ul>
                </div>

                <figure style="text-align: center; margin: 1.5rem 0;">
                    <img src="src/img/2-1-ray-generation.png" alt="Mapping a pixel to a 3D ray" class="clickable" style="max-width: 700px;">
                    <figcaption>▲ Camera-centric view of the mappings implemented below: pixels → camera frame → world rays.</figcaption>
                </figure>

                <h3>Implementation</h3>
                <p>The code mirrors the three conceptual blocks above. First, <code class="code-inline">pixel_to_camera</code> undoes the projection equation <code class="code-inline">s · [u, v, 1]^⊤ = K · [x_c, y_c, z_c]^⊤</code>. Then <code class="code-inline">pixel_to_ray</code> rotates and translates those points into world space and subtracts the camera origin to obtain directions. Finally, <code class="code-inline">get_rays</code> packages everything for a full-resolution image by creating a dense grid of pixel centers.</p>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Ray Generation Functions (PyTorch)</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">def pixel_to_camera(K, uv, s, device='cpu'):
    if not isinstance(K, torch.Tensor):
        K = torch.FloatTensor(K).to(device)
    if not isinstance(uv, torch.Tensor):
        uv = torch.FloatTensor(uv).to(device)

    u, v = uv[:, 0], uv[:, 1]
    f_x, f_y = K[0, 0], K[1, 1]
    o_x, o_y = K[0, 2], K[1, 2]

    x_c = (u - o_x) / f_x * s
    y_c = (v - o_y) / f_y * s
    z_c = torch.ones_like(u) * s
    return torch.stack([x_c, y_c, z_c], dim=-1)

def pixel_to_ray(K, c2w, uv):
    if not isinstance(c2w, torch.Tensor):
        c2w = torch.tensor(c2w, dtype=torch.float32)

    device = c2w.device
    x_c = pixel_to_camera(K, uv, s=1.0, device=device)
    R, t = c2w[:3, :3], c2w[:3, 3]

    x_w = (R @ x_c.T).T + t.unsqueeze(0)
    r_o = t.unsqueeze(0).expand_as(x_w)
    r_d = x_w - r_o
    r_d = r_d / torch.norm(r_d, dim=-1, keepdim=True)
    return r_o, r_d

def get_rays(H, W, focal, c2w):
    device = c2w.device if isinstance(c2w, torch.Tensor) else 'cpu'
    K = torch.FloatTensor([
        [focal, 0, W / 2],
        [0, focal, H / 2],
        [0, 0, 1]
    ]).to(device)

    i, j = torch.meshgrid(
        torch.arange(H, device=device),
        torch.arange(W, device=device),
        indexing='ij'
    )
    uv = torch.stack([j + 0.5, i + 0.5], dim=-1).reshape(-1, 2).float()
    return pixel_to_ray(K, c2w, uv)</code></pre>
                        </div>
                    </details>
                </div>
            </div>
        </section>

        <section id="part2-2" class="info-section">
            <div class="content-centered">
                <h2>Part 2.2: Point Sampling</h2>
                <p>Now that every pixel is a ray, we have to choose a subset of rays per batch and discretize each ray into 3D samples for the NeRF.</p>

                <h3>Sampling Rays Across Images</h3>
                <p>I reuse the Part&nbsp;1 pixel sampler but extend it across all views: pick <code class="code-inline">M</code> images, draw <code class="code-inline">N/M</code> pixels from each, and convert them to rays. Adding <code class="code-inline">+0.5</code> keeps rays centered on the pixels.</p>

<pre><code class="language-python">n_rays = 1024
select_inds = np.random.choice(len(rays_o), size=n_rays, replace=False)
rays_o = rays_o[select_inds]
rays_d = rays_d[select_inds]

pts, z_vals = sample_along_rays(rays_o, rays_d, n_samples=64)

print(f'Points shape: {pts.shape}')
print(f'Z values shape: {z_vals.shape}')
print(f'Z values range: [{z_vals.min():.2f}, {z_vals.max():.2f}]')
print(f'First point: {pts[0, 0]}')</code></pre>

                <h3>Sampling Points Along Rays</h3>
                <p>Each ray is continuous, so I slice it into <code class="code-inline">n_samples</code> depths between <code class="code-inline">near=2.0</code> and <code class="code-inline">far=6.0</code>. During training I jitter inside each interval (stratified sampling) so the network doesn’t overfit to a single lattice.</p>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Stratified Sampling Along Rays</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">def sample_along_rays(rays_o, rays_d, near=2.0, far=6.0,
                      n_samples=64, perturb=True):
    N = rays_o.shape[0]

    # Linear samples in normalized depth, then map to [near, far]
    t_vals = torch.linspace(0., 1., n_samples, device=rays_o.device)
    z_vals = near * (1. - t_vals) + far * t_vals          # (n_samples,)
    z_vals = z_vals.expand(N, n_samples)                  # (N, n_samples)

    if perturb:
        mids = 0.5 * (z_vals[:, :-1] + z_vals[:, 1:])
        upper = torch.cat([mids, z_vals[:, -1:]], dim=-1)
        lower = torch.cat([z_vals[:, :1], mids], dim=-1)
        t_rand = torch.rand_like(z_vals)
        z_vals = lower + (upper - lower) * t_rand

    pts = rays_o.unsqueeze(1) + rays_d.unsqueeze(1) * z_vals.unsqueeze(2)
    return pts, z_vals</code></pre>
                        </div>
                    </details>
                </div>

                <p>The function returns both the sampled 3D coordinates and their depths. Later, the renderer uses <code class="code-inline">z_vals</code> to compute alpha weights and volume accumulation. I typically choose <code class="code-inline">n_samples = 64</code> for training and disable perturbation at evaluation time for deterministic renders.</p>
            </div>
        </section>

        <section id="part2-3" class="info-section">
            <div class="content-centered">
                <h2>Part 2.3: Data Loading</h2>
                <p>This section is basically Part&nbsp;1’s pixel sampler leveled up: now every batch needs to return ray origins, ray directions, and the associated RGB colors across all camera views.</p>

                <h3>Multiview Rays Dataloader</h3>
                <p>I precompute the rays for each image once, flatten everything, and then sample by indexing into those big tensors. This keeps the per-iteration cost tiny and makes it easy to sanity check correspondence between UV coordinates and RGB values.</p>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>RaysData + NerfDataloader (high level)</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">import torch
import numpy as np
from nerf_part2_1_rays import get_rays

class RaysData:
    def __init__(self, images, K, c2ws, device='cpu'):
        if not isinstance(images, torch.Tensor):
            images = torch.FloatTensor(images)
        if not isinstance(c2ws, torch.Tensor):
            c2ws = torch.FloatTensor(c2ws)

        self.images = images.to(device)
        self.c2ws = c2ws.to(device)
        self.device = device
        N, H, W, _ = self.images.shape

        all_rays_o, all_rays_d, all_pixels, all_uvs = [], [], [], []

        for i in range(N):
            rays_o, rays_d = get_rays(H, W, K, self.c2ws[i])
            all_rays_o.append(rays_o)
            all_rays_d.append(rays_d)
            all_pixels.append(self.images[i].reshape(-1, 3))

            y, x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')
            all_uvs.append(torch.stack([x, y], dim=-1).reshape(-1, 2).long())

        self.rays_o = torch.cat(all_rays_o, dim=0)
        self.rays_d = torch.cat(all_rays_d, dim=0)
        self.pixels = torch.cat(all_pixels, dim=0)
        self.uvs = torch.cat(all_uvs, dim=0)

    def sample_rays(self, n):
        indices = torch.randint(0, len(self.rays_o), (n,), device=self.device)
        return self.rays_o[indices], self.rays_d[indices], self.pixels[indices]

class NeRFDataloader:
    def __init__(self, images, c2ws, K_or_focal, H, W, batch_size, device='cpu'):
        if isinstance(K_or_focal, np.ndarray):
            K = K_or_focal
        else:
            focal = float(K_or_focal)
            K = np.array([[focal, 0, W/2], [0, focal, H/2], [0, 0, 1]])
        self.rays_data = RaysData(images, K, c2ws, device)
        self.batch_size = batch_size

    def sample(self):
        return self.rays_data.sample_rays(self.batch_size)</code></pre>
                        </div>
                    </details>
                </div>

                <h3>Validation</h3>
                <p>I used the provided plotter to draw camera frustums and sampled rays. Sampling across all views produces a spiky hedgehog, while sampling from a single pose keeps the rays nicely inside one frustum—perfect for catching any indexing bug.</p>

                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0;">
                    <figure>
                        <img src="src/img/2-3-demo-random-rays.png" alt="Random rays from all views" class="clickable">
                        <figcaption>▲ Random rays from multiple cameras.</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/2-3-demo-rays-from-first-image.png" alt="Rays from first camera" class="clickable">
                        <figcaption>▲ Rays sampled only from the first camera.</figcaption>
                    </figure>
                </div>
            </div>
        </section>

        <section id="part2-4" class="info-section">
            <div class="content-centered">
                <h2>Part 2.4: NeRF Network</h2>
                <p>The network now needs to map 3D locations and view directions to both color and density. Compared to Part&nbsp;1’s tiny MLP, this model is deeper, runs on 3D coordinates, and conditions its RGB prediction on the viewing direction.</p>

                <figure style="text-align: center; margin: 1.5rem 0;">
                    <img src="src/img/2-mlp_nerf.png" alt="NeRF MLP architecture" class="clickable" style="max-width: 900px;">
                    <figcaption>▲ Architecture from the spec</figcaption>
                </figure>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>NeRF model (PyTorch)</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">import torch.nn as nn

class PositionalEncoding(nn.Module):
    def __init__(self, L=10):
        super().__init__()
        self.L = L

    def forward(self, x):
        out = [x]
        for i in range(self.L):
            freq = 2 ** i * np.pi
            out.extend([torch.sin(freq * x), torch.cos(freq * x)])
        return torch.cat(out, dim=-1)

class NeRF(nn.Module):
    def __init__(self, L_pos=10, L_dir=4, hidden_dim=256):
        super().__init__()
        self.pe_pos = PositionalEncoding(L_pos)
        self.pe_dir = PositionalEncoding(L_dir)

        pos_dim = 3 + 3 * 2 * L_pos
        dir_dim = 3 + 3 * 2 * L_dir

        self.lin1 = nn.Linear(pos_dim, hidden_dim)
        self.lin2 = nn.Linear(hidden_dim, hidden_dim)
        self.lin3 = nn.Linear(hidden_dim, hidden_dim)
        self.lin4 = nn.Linear(hidden_dim, hidden_dim)

        self.lin5 = nn.Linear(hidden_dim + pos_dim, hidden_dim)
        self.lin6 = nn.Linear(hidden_dim, hidden_dim)
        self.lin7 = nn.Linear(hidden_dim, hidden_dim)
        self.lin8 = nn.Linear(hidden_dim, hidden_dim)

        self.density_lin = nn.Linear(hidden_dim, 1)
        self.feature_lin = nn.Linear(hidden_dim, hidden_dim)
        self.color_lin1 = nn.Linear(hidden_dim + dir_dim, 128)
        self.color_lin2 = nn.Linear(128, 3)

        nn.init.constant_(self.density_lin.bias, 0.1)

    def forward(self, pts, dirs):
        pts_enc = self.pe_pos(pts)
        dirs_enc = self.pe_dir(dirs)

        x = torch.relu(self.lin1(pts_enc))
        x = torch.relu(self.lin2(x))
        x = torch.relu(self.lin3(x))
        x = torch.relu(self.lin4(x))
        x = torch.cat([x, pts_enc], dim=-1)
        x = torch.relu(self.lin5(x))
        x = torch.relu(self.lin6(x))
        x = torch.relu(self.lin7(x))
        x = torch.relu(self.lin8(x))

        sigma = torch.relu(self.density_lin(x))
        feat = self.feature_lin(x)
        feat_dir = torch.cat([feat, dirs_enc], dim=-1)
        rgb = torch.relu(self.color_lin1(feat_dir))
        rgb = torch.sigmoid(self.color_lin2(rgb))
        return rgb, sigma

model = NeRF(L_pos=10, L_dir=4, hidden_dim=256)</code></pre>
                        </div>
                    </details>
                </div>
            </div>
        </section>

        <section id="part2-5" class="info-section">
            <div class="content-centered">
                <h2>Part 2.5: Volume Rendering</h2>
                <p>This is where the neural outputs turn into actual pixels. Given densities <code class="code-inline">σ</code>, colors <code class="code-inline">c</code>, and depths <code class="code-inline">z</code> sampled along every ray, we integrate them with the discrete volume-rendering equation from the handout:</p>

                <figure style="text-align: center; margin: 1.5rem 0;">
                    <img src="src/img/2-5-formula.jpg" alt="Volume rendering equation" class="clickable" style="max-width: 900px;">
                    <figcaption>▲ Discrete approximation of the continuous volume-rendering integral used in NeRF.</figcaption>
                </figure>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Volume rendering helpers</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">import torch
import numpy as np

def volrend(sigmas, rgbs, step_size):
    N, n_samples = sigmas.shape[:2]
    dists = torch.full((N, n_samples), step_size, device=sigmas.device)
    alpha = 1.0 - torch.exp(-sigmas.squeeze(-1) * dists)
    trans = torch.cumprod(
        torch.cat([torch.ones_like(alpha[:, :1]), 1.0 - alpha[:, :-1]], dim=-1),
        dim=-1
    )
    weights = alpha * trans
    return torch.sum(weights.unsqueeze(-1) * rgbs, dim=1)

def volume_rendering(rgb, sigma, z_vals, rays_d, white_bkgd=False):
    dists = z_vals[:, 1:] - z_vals[:, :-1]
    dists = torch.cat([dists, torch.full_like(dists[:, :1], 1e10)], dim=-1)
    dists = dists * torch.norm(rays_d, dim=-1, keepdim=True)

    alpha = 1.0 - torch.exp(-sigma.squeeze(-1) * dists)
    trans = torch.cumprod(
        torch.cat([torch.ones_like(alpha[:, :1]), 1.0 - alpha[:, :-1]], dim=-1),
        dim=-1
    )
    weights = alpha * trans

    rgb_map = torch.sum(weights.unsqueeze(-1) * rgb, dim=1)
    depth_map = torch.sum(weights * z_vals, dim=-1)
    acc_map = torch.sum(weights, dim=-1)

    if white_bkgd:
        rgb_map = rgb_map + (1.0 - acc_map.unsqueeze(-1))
    return rgb_map, depth_map, acc_map</code></pre>
                        </div>
                    </details>
                </div>

                <h3>Run Part 2 on the Lego Dataset</h3>
                <p>I packaged all subcomponents into a single script that reproduces the NeRF paper’s Lego experiment. It handles argument parsing, ray sampling, training, validation renders, and novel-view generation.</p>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>CLI + imports</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">import os
import sys

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, default='lego',
                        choices=['lego', 'lafufu', 'mydata', 'rabbit', 'miffy', 'all'])
    parser.add_argument('--multi_gpu', action='store_true')
    parser.add_argument('--gpu', type=str, default=None,
                        help='GPU device ID (e.g., "0" or "0,1,2")')
    args = parser.parse_args()

    if args.gpu is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
        print(f'Using GPU(s): {args.gpu}')
else:
    args = None

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm

from nerf_part2_1_rays import get_rays
from nerf_part2_2_sampling import sample_along_rays
from nerf_part2_3_dataloader import NeRFDataloader
from nerf_part2_4_network import NeRF
from nerf_part2_5_rendering import volume_rendering</code></pre>
                        </div>
                    </details>
                </div>

                <div class="code-block">
                    <details>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▶</span>
                            <strong>Rendering helpers</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">def render_rays(model, rays_o, rays_d, n_samples, near, far,
               device, white_bkgd=True, perturb=False):
    pts, z_vals = sample_along_rays(rays_o, rays_d, near, far,
                                    n_samples, perturb=perturb)

    N, S = pts.shape[:2]
    pts_flat = pts.reshape(-1, 3)
    dirs_flat = rays_d.unsqueeze(1).expand(-1, S, -1).reshape(-1, 3)

    rgb, sigma = model(pts_flat, dirs_flat)
    rgb = rgb.reshape(N, S, 3)
    sigma = sigma.reshape(N, S, 1)

    return volume_rendering(rgb, sigma, z_vals, rays_d,
                            white_bkgd=white_bkgd)

def render_image(model, c2w, H, W, K_or_focal, chunk=1024,
                 n_samples=64, near=2.0, far=6.0, device='cpu',
                 white_bkgd=True):
    rays_o, rays_d = get_rays(H, W, K_or_focal, c2w)
    rays_o, rays_d = rays_o.to(device), rays_d.to(device)

    rgb_chunks = []
    for i in range(0, len(rays_o), chunk):
        rgb, _, _ = render_rays(model,
                                rays_o[i:i+chunk],
                                rays_d[i:i+chunk],
                                n_samples, near, far,
                                device=device, white_bkgd=white_bkgd,
                                perturb=False)
        rgb_chunks.append(rgb.cpu())

    rgb_map = torch.cat(rgb_chunks, dim=0).reshape(H, W, 3)
    return rgb_map.numpy()</code></pre>
                        </div>
                    </details>
                </div>

                <div class="code-block">
                    <details>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▶</span>
                            <strong>Training + evaluation utilities</strong>
                        </summary>
                        <div class="code-content">
<pre><code class="language-python">def train_nerf(data_path, n_iters, batch_size, lr,
              n_samples, near, far, save_dir,
              multi_gpu=False, white_bkgd=True):
    os.makedirs(save_dir, exist_ok=True)
    device = torch.device('cuda' if torch.cuda.is_available()
                          else 'mps' if torch.backends.mps.is_available()
                          else 'cpu')
    data = np.load(data_path)
    images_train = data['images_train'] / 255.0
    c2ws_train = data['c2ws_train']
    images_val = data['images_val'] / 255.0
    c2ws_val = data['c2ws_val']
    K = data['K'] if 'K' in data else np.array([
        [data['focal'], 0, images_train.shape[2] / 2],
        [0, data['focal'], images_train.shape[1] / 2],
        [0, 0, 1]
    ])
    H, W = images_train.shape[1:3]

    model = NeRF(L_pos=10, L_dir=4, hidden_dim=256)
    if multi_gpu and torch.cuda.device_count() > 1:
        model = nn.DataParallel(model)
    model = model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    dataloader = NeRFDataloader(images_train, c2ws_train, K, H, W,
                                batch_size, device)

    psnr_history = []
    save_iters = [0, 100, 500, 1000, 2000, n_iters - 1]
    for it in tqdm(range(n_iters)):
        model.train()
        rays_o, rays_d, target_rgb = dataloader.sample()
        pred_rgb, _, _ = render_rays(model, rays_o, rays_d,
                                     n_samples, near, far,
                                     device=device, white_bkgd=white_bkgd,
                                     perturb=True)
        loss = criterion(pred_rgb, target_rgb)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        psnr = -10 * torch.log10(loss)
        psnr_history.append(psnr.item())

        if it in save_iters:
            eval_model = model.module if hasattr(model, 'module') else model
            eval_model.eval()
            with torch.no_grad():
                val_img = render_image(
                    eval_model,
                    torch.FloatTensor(c2ws_val[0]),
                    H, W, K,
                    n_samples=n_samples,
                    near=near, far=far,
                    device=device
                )
                plt.figure(figsize=(10, 5))
                plt.subplot(1, 2, 1)
                plt.imshow(images_val[0]); plt.title('Ground Truth'); plt.axis('off')
                plt.subplot(1, 2, 2)
                plt.imshow(np.clip(val_img, 0, 1))
                plt.title(f'Predicted (iter {it})'); plt.axis('off')
                plt.tight_layout()
                plt.savefig(f'{save_dir}/val_{it:04d}.png', dpi=150)
                plt.close()

    plt.figure(figsize=(10, 6))
    plt.plot(psnr_history); plt.xlabel('Iteration')
    plt.ylabel('PSNR (dB)'); plt.grid(True)
    plt.savefig(f'{save_dir}/psnr_curve.png', dpi=150); plt.close()

    model_to_save = model.module if hasattr(model, 'module') else model
    torch.save(model_to_save.state_dict(), f'{save_dir}/model.pth')
    return model, psnr_history

def render_validation_views(model, data_path, save_dir,
                            n_samples, near, far, device):
    os.makedirs(save_dir, exist_ok=True)
    data = np.load(data_path)
    images_val = data['images_val'] / 255.0
    c2ws_val = data['c2ws_val']
    K = data['K'] if 'K' in data else np.array([
        [data['focal'], 0, images_val.shape[2] / 2],
        [0, data['focal'], images_val.shape[1] / 2],
        [0, 0, 1]
    ])
    H, W = images_val.shape[1:3]

    eval_model = model.module if hasattr(model, 'module') else model
    eval_model.eval()
    with torch.no_grad():
        for i in range(len(images_val)):
            pred_img = render_image(eval_model,
                                    torch.FloatTensor(c2ws_val[i]),
                                    H, W, K,
                                    n_samples=n_samples,
                                    near=near, far=far,
                                    device=device)
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))
            axes[0].imshow(images_val[i]); axes[0].set_title('GT'); axes[0].axis('off')
            axes[1].imshow(np.clip(pred_img, 0, 1))
            axes[1].set_title('Prediction'); axes[1].axis('off')
            axes[2].imshow(np.abs(images_val[i] - pred_img))
            axes[2].set_title('Error'); axes[2].axis('off')
            plt.tight_layout()
            plt.savefig(f'{save_dir}/val_{i:02d}.png', dpi=150, bbox_inches='tight')
            plt.close()

def run_lego(multi_gpu=False):
    save_dir = 'images/tmp/nerf'
    data_path = 'dataset/lego_200x200.npz'
    model, psnr_history = train_nerf(
        data_path=data_path,
        n_iters=5000,
        batch_size=10000,
        lr=5e-4,
        n_samples=64,
        near=2.0,
        far=6.0,
        save_dir=save_dir,
        multi_gpu=multi_gpu
    )
    device = torch.device('cuda' if torch.cuda.is_available()
                          else 'mps' if torch.backends.mps.is_available()
                          else 'cpu')
    render_validation_views(model, data_path, save_dir,
                            n_samples=64, near=2.0, far=6.0, device=device)
    render_novel_views(model, data_path, num_frames=120,
                       save_path=f'{save_dir}/novel_views.gif',
                       n_samples=64, near=2.0, far=6.0, device=device)
    print(f'Final PSNR: {psnr_history[-1]:.2f} dB')</code></pre>
                        </div>
                    </details>
                </div>

                <h3>Results</h3>
                <p>Using the Lego dataset (200×200), I trained with the following settings:</p>
                <div class="result-box" style="margin-bottom: 1rem;">
                    <ul style="margin: 0 0 0 1.25rem;">
                        <li><strong>Iterations:</strong> 5,000</li>
                        <li><strong>Batch size:</strong> 10,000 rays</li>
                        <li><strong>Samples per ray:</strong> 64 (<code class="code-inline">near=2.0</code>, <code class="code-inline">far=6.0</code>)</li>
                        <li><strong>Model:</strong> Same NeRF MLP from Part 2.4 with <code class="code-inline">L_pos=10</code>, <code class="code-inline">L_dir=4</code></li>
                        <li><strong>Optimizer:</strong> Adam @ <code class="code-inline">5e-4</code>, white background</li>
                    </ul>
                </div>
                <p>The final model reaches >23&nbsp;dB PSNR. Below are the intermediate and final visualizations.</p>

                <figure style="text-align: center; margin: 1.25rem 0;">
                    <img src="src/img/2-5-lego-training-preds.png" alt="Lego training snapshots" class="clickable" style="max-width: 100%;">
                    <figcaption>▲ Predicted validation crops at iter 0 → 4999.</figcaption>
                </figure>

                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); gap: 1rem; margin-bottom: 1.5rem;">
                    <figure style="margin: 0; display: flex; flex-direction: column; align-items: center;">
                        <img src="src/img/2-lego/nerf/rays_vis.png" alt="Ray visualization" class="clickable" style="height: 340px; width: auto; object-fit: contain;">
                        <figcaption style="margin-top: 0.5rem;">▲ Rays + samples in viser (all cameras).</figcaption>
                    </figure>
                    <figure style="margin: 0; display: flex; flex-direction: column; align-items: center;">
                        <img src="src/img/2-lego/nerf/psnr_curve.png" alt="PSNR curve" class="clickable" style="height: 340px; width: auto; object-fit: contain;">
                        <figcaption style="margin-top: 0.5rem;">▲ PSNR curve (validation view) during training.</figcaption>
                    </figure>
                </div>

                <p><strong>Validation set renders:</strong></p>
                <div style="display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 0.75rem; margin-bottom: 1.5rem;">
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_00.png" alt="Validation 00" class="clickable">
                        <figcaption>Val 00</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_01.png" alt="Validation 01" class="clickable">
                        <figcaption>Val 01</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_02.png" alt="Validation 02" class="clickable">
                        <figcaption>Val 02</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_03.png" alt="Validation 03" class="clickable">
                        <figcaption>Val 03</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_04.png" alt="Validation 04" class="clickable">
                        <figcaption>Val 04</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_05.png" alt="Validation 05" class="clickable">
                        <figcaption>Val 05</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_06.png" alt="Validation 06" class="clickable">
                        <figcaption>Val 06</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_07.png" alt="Validation 07" class="clickable">
                        <figcaption>Val 07</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_08.png" alt="Validation 08" class="clickable">
                        <figcaption>Val 08</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-lego/nerf_results/val_09.png" alt="Validation 09" class="clickable">
                        <figcaption>Val 09</figcaption>
                    </figure>
                </div>

                <figure style="text-align: center; margin: 1.25rem 0;">
                    <img src="src/img/2-lego/nerf/novel_views_fixed.gif" alt="Lego novel view GIF" class="clickable" style="max-width: 600px;height: 340px;">
                    <figcaption>▲ Novel-view animation (circular camera path) rendered from the trained NeRF.</figcaption>
                </figure>

            </div>
        </section>

        <section id="part2-6" class="info-section">
            <div class="content-centered">
                <h2>Part 2.6: Training with Custom Data</h2>
                <p>I trained NeRF on my custom <strong>Miffy</strong> capture. I reused the same model architecture from Part&nbsp;2.4 (identical NeRF MLP with <code class="code-inline">L_pos=10</code>, <code class="code-inline">L_dir=4</code>) and adjusted the training ranges to match the smaller capture volume.</p>

                <div class="result-box" style="margin-bottom: 1rem;">
                    <ul style="margin: 0 0 0 1.25rem;">
                        <li><strong>Iterations:</strong> 8,000</li>
                        <li><strong>Batch size:</strong> 8,192 rays</li>
                        <li><strong>Samples per ray:</strong> 128 (<code class="code-inline">near=0.05</code>, <code class="code-inline">far=0.65</code>)</li>
                        <li><strong>Optimizer:</strong> Adam @ <code class="code-inline">5e-4</code>, white background</li>
                    </ul>
                </div>

                <figure style="text-align: center; margin: 1.25rem 0;">
                    <img src="src/img/2-6-miffy-training-preds.png" alt="Miffy training predictions" class="clickable" style="max-width: 100%;">
                    <figcaption>▲ Predicted validation crops (iters 0 → 7999) for the Miffy dataset.</figcaption>
                </figure>

                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); gap: 1rem; margin-bottom: 1.5rem;">
                    <figure style="margin: 0; display: flex; flex-direction: column; align-items: center;">
                        <img src="src/img/2-miffy_two_fixed/psnr_curve.png" alt="Miffy PSNR curve" class="clickable" style="height: 260px; width: auto; object-fit: contain;">
                        <figcaption style="margin-top: 0.5rem;">▲ PSNR curve on the custom validation view.</figcaption>
                    </figure>
                </div>

                <p><strong>Validation set renders:</strong></p>
                <div style="display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 0.75rem; margin-bottom: 1.5rem;">
                    <figure style="margin: 0;">
                        <img src="src/img/2-miffy_two_fixed/val_00.png" alt="Miffy Validation 00" class="clickable">
                        <figcaption>Val 00</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-miffy_two_fixed/val_01.png" alt="Miffy Validation 01" class="clickable">
                        <figcaption>Val 01</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-miffy_two_fixed/val_02.png" alt="Miffy Validation 02" class="clickable">
                        <figcaption>Val 02</figcaption>
                    </figure>
                    <figure style="margin: 0;">
                        <img src="src/img/2-miffy_two_fixed/val_03.png" alt="Miffy Validation 03" class="clickable">
                        <figcaption>Val 03</figcaption>
                    </figure>
                </div>

                <figure style="text-align: center; margin: 1.25rem 0;">
                    <img src="src/img/2-miffy_two_fixed/novel_views.gif" alt="Miffy novel views" class="clickable" style="max-width: 600px;">
                    <figcaption>▲ Novel-view animation rendered from the custom Miffy NeRF.</figcaption>
                </figure>

                <div class="observation-box">
                    <h4>Debug Notes</h4>
                    <p>I actually shot five different datasets before this run finally converged. The Lafufu set worked from day one, but my captures kept collapsing because the detector latched onto different ArUco IDs in the same sequence. In hindsight the issue was obvious: I taped down a sheet with <strong>six</strong> tags, so depending on perspective the “active” tag would change mid-frame.</p>
                    <figure style="text-align: center; margin: 1rem 0;">
                        <img src="src/img/2-6-miffy-debug.png" alt="Miffy debug collage" class="clickable" style="max-width: 100%;">
                        <figcaption>▲ Debug frames showing the tag ID jump when multiple tags are visible.</figcaption>
                    </figure>
                    <p>Once I re-shot with <strong>a single tag</strong> in the scene the pipeline behaved—proof that capture setup matters as much as the NeRF code. (Still not sure why the staff’s Lafufu dataset works with six tags.)</p>
                </div>

                <p>All assets for this run live under <code class="code-inline">src/img/2-miffy_two_fixed/</code>; every thumbnail here can be clicked to open the lightbox.</p>
            </div>
        </section>

    </main>

    <!-- Lightbox -->
    <div id="lightbox" class="lightbox">
        <img id="lightbox-img">
        <span class="close">&times;</span>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        // Toggle code blocks
        function toggleCode(element) {
            const codeContent = element.nextElementSibling;
            const toggleIcon = element.querySelector('.toggle-icon');
            if (codeContent.style.display === 'none') {
                codeContent.style.display = 'block';
                toggleIcon.textContent = '▼';
            } else {
                codeContent.style.display = 'none';
                toggleIcon.textContent = '▶';
            }
        }

        // Image lightbox
        document.querySelectorAll('.clickable').forEach(img => {
            img.addEventListener('click', function() {
                const lightbox = document.getElementById('lightbox');
                const lightboxImg = document.getElementById('lightbox-img');
                lightbox.style.display = 'flex';
                lightboxImg.src = this.src;
            });
        });

        document.querySelector('.lightbox .close').addEventListener('click', function() {
            document.getElementById('lightbox').style.display = 'none';
        });

        document.getElementById('lightbox').addEventListener('click', function(e) {
            if (e.target === this) {
                this.style.display = 'none';
            }
        });

        // Smooth scroll
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4: Neural Radiance Fields (NeRF)</title>
    <link rel="stylesheet" href="css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Space+Grotesk:wght@400;500;600;700&family=JetBrains+Mono&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <!-- Header -->
    <header>
        <div class="header-content">
            <div class="header-info">
                <a href="../index.html" class="back-link">← Back to Projects</a>
                <h1>Neural Radiance Fields (NeRF)</h1>
                <p class="subtitle">CS 180 Project 4 · Song-Ze Yu</p>
            </div>

            <div class="overview-grid">
                <div class="overview-card">
                    <h3>Part 0: Camera Calibration & 3D Scan</h3>
                    <p>Calibrate camera using ArUco markers, capture 3D object scan, estimate camera poses, and prepare dataset.</p>
                    <div class="part-nav-grid">
                        <a href="#part0-1" class="part-nav-item">
                            <span class="part-nav-number">0.1</span>
                            <span class="part-nav-title">Calibration</span>
                        </a>
                        <a href="#part0-2" class="part-nav-item">
                            <span class="part-nav-number">0.2</span>
                            <span class="part-nav-title">3D Scan</span>
                        </a>
                        <a href="#part0-3" class="part-nav-item">
                            <span class="part-nav-number">0.3</span>
                            <span class="part-nav-title">Pose</span>
                        </a>
                        <a href="#part0-4" class="part-nav-item">
                            <span class="part-nav-number">0.4</span>
                            <span class="part-nav-title">Dataset</span>
                        </a>
                    </div>
                </div>

                <div class="overview-card">
                    <h3>Part 1: Neural Field for 2D Images</h3>
                    <p>Train MLP with positional encoding to represent 2D images as continuous functions.</p>
                    <div class="part-nav-grid">
                        <a href="#part1" class="part-nav-item">
                            <span class="part-nav-number">1</span>
                            <span class="part-nav-title">Implementation & Results</span>
                        </a>
                    </div>
                </div>

                <div class="overview-card">
                    <h3>Part 2: Neural Radiance Field (3D)</h3>
                    <p>Generate rays, sample points, and render novel views through volume rendering.</p>
                    <div class="part-nav-grid">
                        <a href="#part2-1" class="part-nav-item">
                            <span class="part-nav-number">2.1</span>
                            <span class="part-nav-title">Rays</span>
                        </a>
                        <a href="#part2-2" class="part-nav-item">
                            <span class="part-nav-number">2.2</span>
                            <span class="part-nav-title">Sampling</span>
                        </a>
                        <a href="#part2-3" class="part-nav-item">
                            <span class="part-nav-number">2.3</span>
                            <span class="part-nav-title">Data</span>
                        </a>
                        <a href="#part2-4" class="part-nav-item">
                            <span class="part-nav-number">2.4</span>
                            <span class="part-nav-title">Network</span>
                        </a>
                        <a href="#part2-5" class="part-nav-item">
                            <span class="part-nav-number">2.5</span>
                            <span class="part-nav-title">Render</span>
                        </a>
                        <a href="#part2-6" class="part-nav-item">
                            <span class="part-nav-number">2.6</span>
                            <span class="part-nav-title">Custom</span>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Navigation -->
    <nav class="toc">
        <a href="#part0-1">0.1 Calibration</a>
        <a href="#part0-2">0.2 Photos</a>
        <a href="#part0-3">0.3 Pose</a>
        <a href="#part0-4">0.4 Dataset</a>
        <a href="#part1">1 2D NeRF</a>
        <a href="#part2-1">2.1 Rays</a>
        <a href="#part2-2">2.2 Sampling</a>
        <a href="#part2-3">2.3 Data</a>
        <a href="#part2-4">2.4 Network</a>
        <a href="#part2-5">2.5 Render</a>
        <a href="#part2-6">2.6 Custom</a>
    </nav>

    <main>
        <!-- PART 0: CAMERA CALIBRATION & 3D SCAN -->

        <section id="part0-1" class="info-section">
            <div class="content-centered">
                <h2>Part 0.1: Camera Calibration</h2>

                <p>The goal of camera calibration is to estimate the <strong>intrinsic matrix</strong> and <strong>distortion coefficients</strong> of the camera. These parameters are essential for later stages (pose estimation and 3D reconstruction) and must remain constant throughout the entire capture process — this is why the assignment spec emphasizes <strong>not adjusting the camera's focal length/zoom</strong>.</p>

                <p>By capturing multiple images of ArUco calibration tags from different angles and distances, we can solve for the camera's internal parameters using known 3D-to-2D correspondences.</p>

                <figure style="text-align: center; margin-top: 2rem;">
                    <img src="src/img/0-1-tags.jpg" alt="ArUco Tag Calibration" class="clickable" style="width: 70%;">
                    <figcaption>▲ Sample calibration images showing ArUco tag detection from various angles and distances (33 images total)</figcaption>
                </figure>
                <h3>Implementation</h3>

                <div class="observation-box">
                    <h4>Note: OpenCV Version Compatibility</h4>
                    <p>The spec uses syntax for OpenCV <strong>&lt; 4.7.0</strong>. My implementation uses the updated API for newer versions. See <a href="https://stackoverflow.com/questions/76186376/attributeerror-module-cv2-aruco-has-no-attribute-detectmarkers-python" target="_blank">this StackOverflow discussion</a> for details.</p>
                </div>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> Camera Calibration Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def calibrate_camera(img_dir, tag_size=0.06):
    # Updated API for OpenCV >= 4.7.0
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    obj_points = []  # 3D points in real world space
    img_points = []  # 2D points in image plane

    # Define 3D coordinates of ArUco tag corners
    tag_3d = np.array([
        [0, 0, 0],
        [tag_size, 0, 0],
        [tag_size, tag_size, 0],
        [0, tag_size, 0]
    ], dtype=np.float32)

    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))
    img_shape = None

    # Detect tags in all calibration images
    for img_path in img_files:
        img = cv2.imread(img_path)
        if img_shape is None:
            img_shape = img.shape[:2]

        corners, ids, _ = detector.detectMarkers(img)

        if ids is not None:
            for corner in corners:
                obj_points.append(tag_3d)
                img_points.append(corner.reshape(-1, 2))

    # Solve for camera intrinsics and distortion
    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(
        obj_points, img_points, img_shape[::-1], None, None
    )

    return mtx, dist, rvecs, tvecs</code></pre>
                    </div>
                </div>

                <h3>Results</h3>

                <div class="result-box">
                    <p><strong>Images processed:</strong> 33/33 images (100% detection rate)</p>
                    <p><strong>Total tag detections:</strong> 198 tags</p>
                    <p><strong>Reprojection Error:</strong> 0.786 pixels</p>

                    <p style="margin-top: 1.5rem;"><strong>Camera Intrinsic Matrix (K):</strong></p>
                    <pre style="background: white; padding: 0.75rem; border-radius: 4px; margin: 0.5rem 0; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem;">
[[957.34    0.00  795.40]
 [  0.00  949.21  594.72]
 [  0.00    0.00    1.00]]</pre>

                    <p style="margin-top: 1rem;"><strong>Distortion Coefficients:</strong> [k₁, k₂, p₁, p₂, k₃]</p>
                    <pre style="background: white; padding: 0.75rem; border-radius: 4px; margin: 0.5rem 0; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem;">
[0.261, -1.723, 0.018, 0.027, 2.748]</pre>
                </div>


            </div>
        </section>

        <section id="part0-2" class="info-section">
            <div class="content-centered">
                <h2>Part 0.2: Capturing a 3D Object Scan</h2>

                <p>For this part, I captured images of a cute <strong>CalHacks 12.0 Bear</strong> (from the hackathon a few weeks ago!) placed next to a single ArUco tag. I took <strong>38 images</strong> from various angles and distances while maintaining consistent camera settings.</p>

                <figure style="text-align: center;">
                    <img src="src/img/0-2-calbear.jpg" alt="CalHacks Bear 3D Scan" class="clickable" style="width: 60%;">
                    <figcaption>▲ CalHacks 12.0 Bear captured from multiple viewpoints (38 images total)</figcaption>
                </figure>

            </div>
        </section>

        <section id="part0-3" class="info-section">
            <div class="content-centered">
                <h2>Part 0.3: Camera Pose Estimation</h2>

                <p>The goal of this step is to estimate the <strong>extrinsic parameters</strong> (rotation and translation) for each camera pose. This is known as the <strong>Perspective-n-Point (PnP)</strong> problem: given a set of 3D points in world coordinates and their corresponding 2D projections in an image, we solve for the camera's position and orientation in 3D space.</p>

                <h3>Implementation</h3>

                <div class="observation-box">
                    <h4>Important: World-to-Camera vs Camera-to-World</h4>
                    <For>OpenCV's <code class="code-inline">solvePnP()</code> returns the <strong>world-to-camera (w2c)</strong> transformation.<br>For NeRF, we need the <strong>camera-to-world (c2w)</strong> transformation, so we must invert the result:</p>
                    <pre style="background: white; padding: 0.75rem; border-radius: 4px; margin: 0.5rem 0; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem; line-height: 1.8;">
c2w = [R | t] = [R₀₀  R₀₁  R₀₂  tₓ]
      [0 | 1]   [R₁₀  R₁₁  R₁₂  tᵧ]
                [R₂₀  R₂₁  R₂₂  t_z]
                [ 0    0    0    1 ]</pre>
                </div>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> Pose Estimation Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def estimate_camera_poses(img_dir, mtx, dist, tag_size=0.02):
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    # 3D coordinates of single ArUco tag
    tag_3d = np.array([
        [0, 0, 0],
        [tag_size, 0, 0],
        [tag_size, tag_size, 0],
        [0, tag_size, 0]
    ], dtype=np.float32)

    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))
    poses = []
    images = []

    for i, img_path in enumerate(img_files):
        img = cv2.imread(img_path)
        if img is None:
            continue

        corners, ids, _ = detector.detectMarkers(img)

        if ids is not None and len(ids) > 0:
            img_points = corners[0].reshape(-1, 2)

            # Solve PnP for world-to-camera transformation
            success, rvec, tvec = cv2.solvePnP(
                tag_3d, img_points, mtx, dist
            )

            if success:
                R, _ = cv2.Rodrigues(rvec)

                # Build w2c matrix
                w2c = np.eye(4)
                w2c[:3, :3] = R
                w2c[:3, 3] = tvec.flatten()

                # Invert to get c2w for NeRF
                c2w = np.linalg.inv(w2c)

                poses.append({
                    'idx': i,
                    'filename': os.path.basename(img_path),
                    'c2w': c2w
                })
                images.append(img)

    return poses, images</code></pre>
                    </div>
                </div>

                <h3>Visualization with Viser</h3>

                <p>To verify our pose estimation, we visualize all camera frustums in 3D space using <code class="code-inline">viser</code>.</p>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> 3D Frustum Visualization
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def visualize_camera_poses(poses, images, mtx):
    server = viser.ViserServer(share=True)

    H, W = images[0].shape[:2]

    for i, (pose_data, img) in enumerate(zip(poses, images)):
        c2w = pose_data['c2w']
        server.scene.add_camera_frustum(
            f"/cameras/{i}",
            fov=2 * np.arctan2(H / 2, mtx[0, 0]),
            aspect=W / H,
            scale=0.02,
            wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz,
            position=c2w[:3, 3],
            image=img
        )

    try:
        while True:
            time.sleep(0.1)</code></pre>
                    </div>
                </div>

                <h3>Results</h3>

                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 1.5rem 0;">
                    <figure>
                        <img src="src/img/0-3-viser-demo1.jpg" alt="Camera Frustums View 1" class="clickable" style="height: 350px; object-fit: cover;">
                        <figcaption>▲ 3D visualization of camera frustums (view 1)</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/0-3-viser-demo2.jpg" alt="Camera Frustums View 2" class="clickable" style="height: 350px; object-fit: cover;">
                        <figcaption>▲ 3D visualization of camera frustums (view 2)</figcaption>
                    </figure>
                </div>
            </div>
        </section>

        <section id="part0-4" class="info-section">
            <div class="content-centered">
                <h2>Part 0.4: Dataset Preparation</h2>

                <p>The final step is to <strong>undistort all images</strong> and package everything into a dataset format ready for NeRF training. This combines all previous steps from Part 0.1 to 0.3.</p>

                <h3>Implementation</h3>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> Dataset Creation Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def create_nerf_dataset(img_dir, mtx, dist, tag_size=0.06,
                        train_ratio=0.7, val_ratio=0.15,
                        output_path='output/my_data.npz'):
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    tag_3d = np.array([
        [0, 0, 0],
        [tag_size, 0, 0],
        [tag_size, tag_size, 0],
        [0, tag_size, 0]
    ], dtype=np.float32)

    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))
    all_images = []
    all_c2ws = []

    for img_path in img_files:
        img = cv2.imread(img_path)
        if img is None:
            continue

        corners, ids, _ = detector.detectMarkers(img)

        if ids is not None and len(ids) > 0:
            img_points = corners[0].reshape(-1, 2)
            success, rvec, tvec = cv2.solvePnP(tag_3d, img_points, mtx, dist)

            if success:
                # Key steps: undistort and convert color
                undistorted = cv2.undistort(img, mtx, dist)
                undistorted_rgb = cv2.cvtColor(undistorted, cv2.COLOR_BGR2RGB)

                # Compute c2w matrix
                R, _ = cv2.Rodrigues(rvec)
                w2c = np.eye(4)
                w2c[:3, :3] = R
                w2c[:3, 3] = tvec.flatten()
                c2w = np.linalg.inv(w2c)

                all_images.append(undistorted_rgb)
                all_c2ws.append(c2w)

    all_images = np.array(all_images)
    all_c2ws = np.array(all_c2ws)

    # Split into train/val/test
    n_total = len(all_images)
    n_train = int(n_total * train_ratio)
    n_val = int(n_total * val_ratio)

    idx = np.random.permutation(n_total)
    train_idx = idx[:n_train]
    val_idx = idx[n_train:n_train + n_val]
    test_idx = idx[n_train + n_val:]

    focal = mtx[0, 0]

    # Save as .npz
    np.savez(
        output_path,
        images_train=all_images[train_idx],
        c2ws_train=all_c2ws[train_idx],
        images_val=all_images[val_idx],
        c2ws_val=all_c2ws[val_idx],
        c2ws_test=all_c2ws[test_idx],
        focal=focal
    )

    return (all_images[train_idx], all_c2ws[train_idx],
            all_images[val_idx], all_c2ws[val_idx],
            all_c2ws[test_idx], focal)</code></pre>
                    </div>
                </div>
                <h3>Dataset Structure</h3>

                <div class="result-box">
                    <h4>Output: my_data.npz</h4>
                    <p>The packaged dataset contains:</p>
                    <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
                        <li><code class="code-inline">images_train</code>: Training images (undistorted RGB)</li>
                        <li><code class="code-inline">c2ws_train</code>: Camera-to-world matrices for training (0.7)</li>
                        <li><code class="code-inline">images_val</code>: Validation images</li>
                        <li><code class="code-inline">c2ws_val</code>: Camera-to-world matrices for validation (0.15)</li>
                        <li><code class="code-inline">c2ws_test</code>: Camera-to-world matrices for test (0.15)</li>
                        <li><code class="code-inline">focal</code>: Focal length (from intrinsic matrix)</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- PART 1: NEURAL FIELD FOR 2D IMAGES -->

        <section id="part1" class="info-section">
            <div class="content-centered">
                <h2>Part 1: Neural Field for 2D Images</h2>

                <p>Before jumping into 3D NeRF, which learns the mapping <code class="code-inline">F: {x, y, z, θ, φ} → {r, g, b, σ}</code>, we start with 2D as a simpler introduction. In 2D, we only need to solve <code class="code-inline">F: {u, v} → {r, g, b}</code>, where <code class="code-inline">{u, v}</code> represents 2D pixel coordinates.</p>

                <!-- Positional Encoding -->
                <h3>1.1 Sinusoidal Positional Encoding (PE)</h3>

                <p>Positional encoding helps neural networks capture high-frequency details by transforming input coordinates into a higher-dimensional space using sinusoidal functions. Without PE, MLPs struggle to represent fine details in images.</p>

                <p>The encoding applies sine and cosine functions at different frequencies:</p>
                <figure>
                    <img src="src/img/1-pe.jpg" alt="2D Neural Field Architecture" class="clickable">
                </figure>
                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Positional Encoding Implementation</strong>
                        </summary>
                        <div class="code-content">
                            <pre><code class="language-python">class PositionalEncoding(nn.Module):
    def __init__(self, L=10):
        super().__init__()
        self.L = L

    def forward(self, x):
        out = [x]
        for i in range(self.L):
            out.append(torch.sin(2**i * np.pi * x))
            out.append(torch.cos(2**i * np.pi * x))
        return torch.cat(out, dim=-1)</code></pre>
                        </div>
                    </details>
                </div>

                <p>For <code class="code-inline">L=10</code> frequency bands, a 2D coordinate is transformed from dimension 2 to dimension <code class="code-inline">2 + 2×L×2 = 42</code>.</p>

                <!-- MLP Architecture -->
                <h3>1.2 Multilayer Perceptron (MLP) Architecture</h3>

                <p>Our neural field network follows the architecture shown below:</p>

                <figure>
                    <img src="src/img/1-mlp_img.jpg" alt="2D Neural Field Architecture" class="clickable">
                    <figcaption>Network architecture: 2D coordinates → PE → 3 hidden layers (256 units) → RGB output</figcaption>
                </figure>

                <p>The network consists of:</p>
                <ul>
                    <li><strong>Input:</strong> 2D pixel coordinates (u, v)</li>
                    <li><strong>Positional Encoding:</strong> Expands input to 42 dimensions</li>
                    <li><strong>Hidden Layers:</strong> 3 fully-connected layers with 256 units each, using ReLU activation</li>
                    <li><strong>Output Layer:</strong> Maps to RGB values (3 channels) with Sigmoid activation to ensure [0,1] range</li>
                </ul>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Neural Field 2D Implementation</strong>
                        </summary>
                        <div class="code-content">
                            <pre><code class="language-python">class NeuralField2D(nn.Module):
    def __init__(self, L=10, hidden_dim=256):
        super().__init__()
        self.pe = PositionalEncoding(L)
        pe_dim = 2 + 2 * L * 2  # 42 dimensions

        self.mlp = nn.Sequential(
            nn.Linear(pe_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 3),
            nn.Sigmoid()
        )

    def forward(self, coords):
        encoded = self.pe(coords)
        return self.mlp(encoded)</code></pre>
                        </div>
                    </details>
                </div>

                <!-- Dataloader -->
                <h3>1.3 Pixel Dataloader</h3>

                <p>Since high-resolution images are memory-intensive, we implement a dataloader that randomly samples <code class="code-inline">N</code> pixels at every iteration for training. This approach:</p>
                <ul>
                    <li>Returns <code class="code-inline">N×2</code> normalized 2D coordinates in [0,1]</li>
                    <li>Returns corresponding <code class="code-inline">N×3</code> RGB color values in [0,1]</li>
                    <li>Enables efficient mini-batch training without loading entire images</li>
                </ul>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Pixel Dataloader Implementation</strong>
                        </summary>
                        <div class="code-content">
                            <pre><code class="language-python">class PixelDataloader:
    def __init__(self, img, batch_size=10000):
        self.img = img / 255.0  # Normalize to [0,1]
        self.H, self.W = img.shape[:2]
        self.batch_size = batch_size

        # Create normalized coordinate grid
        y, x = np.meshgrid(np.arange(self.H), np.arange(self.W), indexing='ij')
        self.coords = np.stack([x / self.W, y / self.H], axis=-1).reshape(-1, 2)
        self.colors = self.img.reshape(-1, 3)

    def sample(self):
        """Randomly sample batch_size pixels"""
        idx = np.random.choice(len(self.coords), self.batch_size, replace=False)
        coords = torch.FloatTensor(self.coords[idx])
        colors = torch.FloatTensor(self.colors[idx])
        return coords, colors

    def get_all(self):
        """Get all pixels (for validation/visualization)"""
        coords = torch.FloatTensor(self.coords)
        colors = torch.FloatTensor(self.colors)
        return coords, colors</code></pre>
                        </div>
                    </details>
                </div>

                <!-- Training -->
                <h3>1.4 Training: Loss Function, Optimizer, and Metric</h3>

                <p>We train the neural field to minimize the difference between predicted and ground truth pixel colors using:</p>
                <ul>
                    <li><strong>Loss Function:</strong> Mean Squared Error (MSE) between predicted and actual RGB values</li>
                    <li><strong>Metric:</strong> Peak Signal-to-Noise Ratio (PSNR) = <code class="code-inline">10 × log₁₀(1/MSE)</code></li>
                    <li><strong>Optimizer:</strong> Adam with learning rate <code class="code-inline">1e-2</code></li>
                    <li><strong>Training:</strong> 1000-3000 iterations with batch size of 10,000 pixels</li>
                </ul>

                <div class="code-block">
                    <details open>
                        <summary class="code-toggle">
                            <span class="toggle-icon">▼</span>
                            <strong>Training Loop Implementation</strong>
                        </summary>
                        <div class="code-content">
                            <pre><code class="language-python">def train_neural_field(img_path, L=10, hidden_dim=256, lr=1e-2, iters=3000, batch_size=10000, save_dir='images'):
    os.makedirs(save_dir, exist_ok=True)

    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
    print(f'Using device: {device}')

    img = skio.imread(img_path)
    if img.ndim == 2:
        img = np.stack([img] * 3, axis=-1)

    H, W = img.shape[:2]

    model = NeuralField2D(L=L, hidden_dim=hidden_dim).to(device)
    dataloader = PixelDataloader(img, N=batch_size)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    psnr_history = []
    save_iters = [0, 100, 500, 1000, 2000, iters-1]

    for it in range(iters):
        coords, colors = dataloader.sample()
        coords, colors = coords.to(device), colors.to(device)

        pred_colors = model(coords)
        loss = criterion(pred_colors, colors)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        psnr = psnr_from_mse(loss)
        psnr_history.append(psnr.item())

        if it % 100 == 0 or it == iters - 1:
            print(f'Iter {it:4d}: Loss={loss.item():.6f}, PSNR={psnr.item():.2f} dB')

        # Save visualization at key iterations
        if it in save_iters:
            with torch.no_grad():
                y, x = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')
                test_coords = np.stack([x / W, y / H], axis=-1).reshape(-1, 2)
                test_coords = torch.FloatTensor(test_coords).to(device)

                pred = []
                for i in range(0, len(test_coords), batch_size):
                    pred.append(model(test_coords[i:i+batch_size]).cpu())
                pred = torch.cat(pred, dim=0)

                pred_img = pred.reshape(H, W, 3).numpy()

                # Save comparison: GT, Prediction, Error
                plt.figure(figsize=(12, 4))
                plt.subplot(1, 3, 1)
                plt.imshow(img / 255.0)
                plt.title('Ground Truth')
                plt.axis('off')

                plt.subplot(1, 3, 2)
                plt.imshow(pred_img)
                plt.title(f'Prediction (iter {it})')
                plt.axis('off')

                plt.subplot(1, 3, 3)
                plt.imshow(np.abs(img / 255.0 - pred_img))
                plt.title('Error')
                plt.axis('off')

                plt.tight_layout()
                plt.savefig(f'{save_dir}/iter_{it:04d}.png', bbox_inches='tight', dpi=150)
                plt.close()

    # Save PSNR curve
    plt.figure(figsize=(10, 6))
    plt.plot(psnr_history)
    plt.xlabel('Iteration')
    plt.ylabel('PSNR (dB)')
    plt.title('Training PSNR')
    plt.grid(True)
    plt.savefig(f'{save_dir}/psnr_curve.png', bbox_inches='tight', dpi=150)
    plt.close()

    return model, psnr_history</code></pre>
                        </div>
                    </details>
                </div>

                <!-- Results -->
                <h3>1.5 Results and Analysis</h3>

                <h4>1) Model Architecture</h4>
                <div class="result-box">
                    <ul>
                        <li> <strong>Positional Encoding:</strong> L = 10 frequency bands (input dimension: 42)</li>
                        <li> <strong>Network Depth:</strong> 3 hidden layers</li>
                        <li> <strong>Layer Width:</strong> 256 units per hidden layer</li>
                        <li> <strong>Activation:</strong> ReLU (hidden layers), Sigmoid (output layer)</li>
                        <li> <strong>Learning Rate:</strong> 1e-2 (Adam optimizer)</li>
                        <li> <strong>Batch Size:</strong> 10,000 pixels per iteration</li>
                        <li> <strong>Training Iterations:</strong> 3,000</li>
                    </ul>
                </div>

                <h4>2) Training Progression</h4>
                <p>The following images show how the neural field learns to reconstruct the image over time. </p>

                <p style="margin-bottom: 0.5rem;"><strong>Test Image (Fox):</strong></p>
                <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 1rem; margin: 0.5rem 0 1.5rem 0;">
                    <figure>
                        <img src="src/img/1-fox/iter_0000.png" alt="Fox - Iteration 0" class="clickable">
                        <figcaption>Iteration 0</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-fox/iter_0100.png" alt="Fox - Iteration 100" class="clickable">
                        <figcaption>Iteration 100</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-fox/iter_0500.png" alt="Fox - Iteration 500" class="clickable">
                        <figcaption>Iteration 500</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-fox/iter_1000.png" alt="Fox - Iteration 1000" class="clickable">
                        <figcaption>Iteration 1000</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-fox/iter_2000.png" alt="Fox - Iteration 2000" class="clickable">
                        <figcaption>Iteration 2000</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-fox/iter_2999.png" alt="Fox - Iteration 2999" class="clickable">
                        <figcaption>Iteration 2999 (Final)</figcaption>
                    </figure>
                </div>

                <p style="margin-bottom: 0.5rem;"><strong>My Own Image (CalHacks Bear):</strong></p>
                <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 1rem; margin: 0.5rem 0 1.5rem 0;">
                    <figure>
                        <img src="src/img/1-calbear/iter_0000.png" alt="CalBear - Iteration 0" class="clickable">
                        <figcaption>Iteration 0</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-calbear/iter_0100.png" alt="CalBear - Iteration 100" class="clickable">
                        <figcaption>Iteration 100</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-calbear/iter_0500.png" alt="CalBear - Iteration 500" class="clickable">
                        <figcaption>Iteration 500</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-calbear/iter_1000.png" alt="CalBear - Iteration 1000" class="clickable">
                        <figcaption>Iteration 1000</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-calbear/iter_2000.png" alt="CalBear - Iteration 2000" class="clickable">
                        <figcaption>Iteration 2000</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-calbear/iter_2999.png" alt="CalBear - Iteration 2999" class="clickable">
                        <figcaption>Iteration 2999 (Final)</figcaption>
                    </figure>
                </div>

                <h4>3) Hyperparameter Tuning</h4>
                <p>We experiment with different values of L and layer width to understand their impact on reconstruction quality. <br>Using very low values reveals how these hyperparameters affect the model's ability to capture fine details.</p>

                <p style="margin-bottom: 0.5rem;"><strong>Fox Image - Hyperparameter Comparison:</strong></p>
                <figure>
                    <img src="src/img/1-fox/hyperparam_comparison.png" alt="Fox - Hyperparameter Grid" class="clickable" style="max-height: 60vh; width: auto;">
                    <figcaption>2×2 grid comparing different L values and layer widths on the fox image</figcaption>
                </figure>

                <p style="margin-bottom: 0.5rem;"><strong>CalHacks Bear - Hyperparameter Comparison:</strong></p>
                <figure>
                    <img src="src/img/1-calbear/hyperparam_comparison.png" alt="CalBear - Hyperparameter Grid" class="clickable" style="max-height: 60vh; width: auto;">
                    <figcaption>2×2 grid comparing different L values and layer widths on the CalHacks bear</figcaption>
                </figure>

                <div class="observation-box">
                    <h4>Observations</h4>
                    <ul>
                        <li><strong>Positional Encoding (L):</strong> Lower L values struggle to capture high-frequency details, resulting in blurry outputs. Higher L values enable the network to represent fine textures and edges.</li>
                        <li><strong>Layer Width:</strong> Narrower networks (fewer neurons per layer) have limited capacity, leading to smoother, less detailed reconstructions. Wider networks can fit more complex patterns.</li>
                    </ul>
                </div>

                <h4>4) Training PSNR Curves</h4>
                <p>This shows reconstruction quality improvement during training. Higher PSNR indicates better image quality.</p>

                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 1.5rem; margin: 1.5rem 0;">
                    <figure>
                        <img src="src/img/1-fox/psnr_curve.png" alt="Fox PSNR Curve" class="clickable">
                        <figcaption>Fox training PSNR progression</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/1-calbear/psnr_curve.png" alt="CalBear PSNR Curve" class="clickable">
                        <figcaption>CalHacks bear training PSNR progression</figcaption>
                    </figure>
                </div>

            </div>
        </section>

        <!-- PART 2: NEURAL RADIANCE FIELD (3D) -->

        <section id="part2-1" class="info-section">
            <div class="content-centered">
                <h2>Part 2.1: Ray Generation</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-2" class="info-section">
            <div class="content-centered">
                <h2>Part 2.2: Point Sampling</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-3" class="info-section">
            <div class="content-centered">
                <h2>Part 2.3: Data Loading</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-4" class="info-section">
            <div class="content-centered">
                <h2>Part 2.4: NeRF Network</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-5" class="info-section">
            <div class="content-centered">
                <h2>Part 2.5: Volume Rendering</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-6" class="info-section">
            <div class="content-centered">
                <h2>Part 2.6: Training with Custom Data</h2>
                <p>[Your content here]</p>
            </div>
        </section>

    </main>

    <!-- Lightbox -->
    <div id="lightbox" class="lightbox">
        <img id="lightbox-img">
        <span class="close">&times;</span>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        // Toggle code blocks
        function toggleCode(element) {
            const codeContent = element.nextElementSibling;
            const toggleIcon = element.querySelector('.toggle-icon');
            if (codeContent.style.display === 'none') {
                codeContent.style.display = 'block';
                toggleIcon.textContent = '▼';
            } else {
                codeContent.style.display = 'none';
                toggleIcon.textContent = '▶';
            }
        }

        // Image lightbox
        document.querySelectorAll('.clickable').forEach(img => {
            img.addEventListener('click', function() {
                const lightbox = document.getElementById('lightbox');
                const lightboxImg = document.getElementById('lightbox-img');
                lightbox.style.display = 'flex';
                lightboxImg.src = this.src;
            });
        });

        document.querySelector('.lightbox .close').addEventListener('click', function() {
            document.getElementById('lightbox').style.display = 'none';
        });

        document.getElementById('lightbox').addEventListener('click', function(e) {
            if (e.target === this) {
                this.style.display = 'none';
            }
        });

        // Smooth scroll
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>

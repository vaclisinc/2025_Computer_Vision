<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4: Neural Radiance Fields (NeRF)</title>
    <link rel="stylesheet" href="css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Space+Grotesk:wght@400;500;600;700&family=JetBrains+Mono&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <!-- Header -->
    <header>
        <div class="header-content">
            <div class="header-info">
                <a href="../index.html" class="back-link">← Back to Projects</a>
                <h1>Neural Radiance Fields (NeRF)</h1>
                <p class="subtitle">CS 180 Project 4 · Song-Ze Yu</p>
            </div>

            <div class="overview-grid">
                <div class="overview-card">
                    <h3>Part 0: Camera Calibration & 3D Scan</h3>
                    <p>Calibrate camera using ArUco markers, capture 3D object scan, estimate camera poses, and prepare dataset.</p>
                    <div class="part-nav-grid">
                        <a href="#part0-1" class="part-nav-item">
                            <span class="part-nav-number">0.1</span>
                            <span class="part-nav-title">Calibration</span>
                        </a>
                        <a href="#part0-2" class="part-nav-item">
                            <span class="part-nav-number">0.2</span>
                            <span class="part-nav-title">3D Scan</span>
                        </a>
                        <a href="#part0-3" class="part-nav-item">
                            <span class="part-nav-number">0.3</span>
                            <span class="part-nav-title">Pose</span>
                        </a>
                        <a href="#part0-4" class="part-nav-item">
                            <span class="part-nav-number">0.4</span>
                            <span class="part-nav-title">Dataset</span>
                        </a>
                    </div>
                </div>

                <div class="overview-card">
                    <h3>Part 1: Neural Field for 2D Images</h3>
                    <p>Train MLP with positional encoding to represent 2D images as continuous functions.</p>
                    <div class="part-nav-grid">
                        <a href="#part1" class="part-nav-item">
                            <span class="part-nav-number">1</span>
                            <span class="part-nav-title">Implementation & Results</span>
                        </a>
                    </div>
                </div>

                <div class="overview-card">
                    <h3>Part 2: Neural Radiance Field (3D)</h3>
                    <p>Generate rays, sample points, and render novel views through volume rendering.</p>
                    <div class="part-nav-grid">
                        <a href="#part2-1" class="part-nav-item">
                            <span class="part-nav-number">2.1</span>
                            <span class="part-nav-title">Rays</span>
                        </a>
                        <a href="#part2-2" class="part-nav-item">
                            <span class="part-nav-number">2.2</span>
                            <span class="part-nav-title">Sampling</span>
                        </a>
                        <a href="#part2-3" class="part-nav-item">
                            <span class="part-nav-number">2.3</span>
                            <span class="part-nav-title">Data</span>
                        </a>
                        <a href="#part2-4" class="part-nav-item">
                            <span class="part-nav-number">2.4</span>
                            <span class="part-nav-title">Network</span>
                        </a>
                        <a href="#part2-5" class="part-nav-item">
                            <span class="part-nav-number">2.5</span>
                            <span class="part-nav-title">Render</span>
                        </a>
                        <a href="#part2-6" class="part-nav-item">
                            <span class="part-nav-number">2.6</span>
                            <span class="part-nav-title">Custom</span>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Navigation -->
    <nav class="toc">
        <a href="#part0-1">0.1 Calibration</a>
        <a href="#part0-2">0.2 3D Scan</a>
        <a href="#part0-3">0.3 Pose</a>
        <a href="#part0-4">0.4 Dataset</a>
        <a href="#part1">1 Neural Field for 2D</a>
        <a href="#part2-1">2.1 Rays</a>
        <a href="#part2-2">2.2 Sampling</a>
        <a href="#part2-3">2.3 Data</a>
        <a href="#part2-4">2.4 Network</a>
        <a href="#part2-5">2.5 Render</a>
        <a href="#part2-6">2.6 Custom</a>
    </nav>

    <main>
        <!-- PART 0: CAMERA CALIBRATION & 3D SCAN -->

        <section id="part0-1" class="info-section">
            <div class="content-centered">
                <h2>Part 0.1: Camera Calibration</h2>

                <p>The goal of camera calibration is to estimate the <strong>intrinsic matrix</strong> and <strong>distortion coefficients</strong> of the camera. These parameters are essential for later stages (pose estimation and 3D reconstruction) and must remain constant throughout the entire capture process — this is why the assignment spec emphasizes <strong>not adjusting the camera's focal length/zoom</strong>.</p>

                <p>By capturing multiple images of ArUco calibration tags from different angles and distances, we can solve for the camera's internal parameters using known 3D-to-2D correspondences.</p>

                <figure style="text-align: center; margin-top: 2rem;">
                    <img src="src/img/0-1-tags.jpg" alt="ArUco Tag Calibration" class="clickable" style="width: 70%;">
                    <figcaption>▲ Sample calibration images showing ArUco tag detection from various angles and distances (33 images total)</figcaption>
                </figure>
                <h3>Implementation</h3>

                <div class="observation-box">
                    <h4>Note: OpenCV Version Compatibility</h4>
                    <p>The spec uses syntax for OpenCV <strong>&lt; 4.7.0</strong>. My implementation uses the updated API for newer versions. See <a href="https://stackoverflow.com/questions/76186376/attributeerror-module-cv2-aruco-has-no-attribute-detectmarkers-python" target="_blank">this StackOverflow discussion</a> for details.</p>
                </div>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> Camera Calibration Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def calibrate_camera(img_dir, tag_size=0.06):
    # Updated API for OpenCV >= 4.7.0
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    obj_points = []  # 3D points in real world space
    img_points = []  # 2D points in image plane

    # Define 3D coordinates of ArUco tag corners
    tag_3d = np.array([
        [0, 0, 0],
        [tag_size, 0, 0],
        [tag_size, tag_size, 0],
        [0, tag_size, 0]
    ], dtype=np.float32)

    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))
    img_shape = None

    # Detect tags in all calibration images
    for img_path in img_files:
        img = cv2.imread(img_path)
        if img_shape is None:
            img_shape = img.shape[:2]

        corners, ids, _ = detector.detectMarkers(img)

        if ids is not None:
            for corner in corners:
                obj_points.append(tag_3d)
                img_points.append(corner.reshape(-1, 2))

    # Solve for camera intrinsics and distortion
    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(
        obj_points, img_points, img_shape[::-1], None, None
    )

    return mtx, dist, rvecs, tvecs</code></pre>
                    </div>
                </div>

                <h3>Results</h3>

                <div class="result-box">
                    <p><strong>Images processed:</strong> 33/33 images (100% detection rate)</p>
                    <p><strong>Total tag detections:</strong> 198 tags</p>
                    <p><strong>Reprojection Error:</strong> 0.786 pixels</p>

                    <p style="margin-top: 1.5rem;"><strong>Camera Intrinsic Matrix (K):</strong></p>
                    <pre style="background: white; padding: 0.75rem; border-radius: 4px; margin: 0.5rem 0; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem;">
[[957.34    0.00  795.40]
 [  0.00  949.21  594.72]
 [  0.00    0.00    1.00]]</pre>

                    <p style="margin-top: 1rem;"><strong>Distortion Coefficients:</strong> [k₁, k₂, p₁, p₂, k₃]</p>
                    <pre style="background: white; padding: 0.75rem; border-radius: 4px; margin: 0.5rem 0; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem;">
[0.261, -1.723, 0.018, 0.027, 2.748]</pre>
                </div>


            </div>
        </section>

        <section id="part0-2" class="info-section">
            <div class="content-centered">
                <h2>Part 0.2: Capturing a 3D Object Scan</h2>

                <p>For this part, I captured images of a cute <strong>CalHacks 12.0 Bear</strong> (from the hackathon a few weeks ago!) placed next to a single ArUco tag. I took <strong>38 images</strong> from various angles and distances while maintaining consistent camera settings.</p>

                <figure style="text-align: center;">
                    <img src="src/img/0-2-calbear.jpg" alt="CalHacks Bear 3D Scan" class="clickable" style="width: 60%;">
                    <figcaption>▲ CalHacks 12.0 Bear captured from multiple viewpoints (38 images total)</figcaption>
                </figure>

            </div>
        </section>

        <section id="part0-3" class="info-section">
            <div class="content-centered">
                <h2>Part 0.3: Camera Pose Estimation</h2>

                <p>The goal of this step is to estimate the <strong>extrinsic parameters</strong> (rotation and translation) for each camera pose. This is known as the <strong>Perspective-n-Point (PnP)</strong> problem: given a set of 3D points in world coordinates and their corresponding 2D projections in an image, we solve for the camera's position and orientation in 3D space.</p>

                <h3>Implementation</h3>

                <div class="observation-box">
                    <h4>Important: World-to-Camera vs Camera-to-World</h4>
                    <For>OpenCV's <code class="code-inline">solvePnP()</code> returns the <strong>world-to-camera (w2c)</strong> transformation.<br>For NeRF, we need the <strong>camera-to-world (c2w)</strong> transformation, so we must invert the result:</p>
                    <pre style="background: white; padding: 0.75rem; border-radius: 4px; margin: 0.5rem 0; font-family: 'JetBrains Mono', monospace; font-size: 0.85rem; line-height: 1.8;">
c2w = [R | t] = [R₀₀  R₀₁  R₀₂  tₓ]
      [0 | 1]   [R₁₀  R₁₁  R₁₂  tᵧ]
                [R₂₀  R₂₁  R₂₂  t_z]
                [ 0    0    0    1 ]</pre>
                </div>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> Pose Estimation Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def estimate_camera_poses(img_dir, mtx, dist, tag_size=0.02):
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    # 3D coordinates of single ArUco tag
    tag_3d = np.array([
        [0, 0, 0],
        [tag_size, 0, 0],
        [tag_size, tag_size, 0],
        [0, tag_size, 0]
    ], dtype=np.float32)

    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))
    poses = []
    images = []

    for i, img_path in enumerate(img_files):
        img = cv2.imread(img_path)
        if img is None:
            continue

        corners, ids, _ = detector.detectMarkers(img)

        if ids is not None and len(ids) > 0:
            img_points = corners[0].reshape(-1, 2)

            # Solve PnP for world-to-camera transformation
            success, rvec, tvec = cv2.solvePnP(
                tag_3d, img_points, mtx, dist
            )

            if success:
                R, _ = cv2.Rodrigues(rvec)

                # Build w2c matrix
                w2c = np.eye(4)
                w2c[:3, :3] = R
                w2c[:3, 3] = tvec.flatten()

                # Invert to get c2w for NeRF
                c2w = np.linalg.inv(w2c)

                poses.append({
                    'idx': i,
                    'filename': os.path.basename(img_path),
                    'c2w': c2w
                })
                images.append(img)

    return poses, images</code></pre>
                    </div>
                </div>

                <h3>Visualization with Viser</h3>

                <p>To verify our pose estimation, we visualize all camera frustums in 3D space using <code class="code-inline">viser</code>.</p>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> 3D Frustum Visualization
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def visualize_camera_poses(poses, images, mtx):
    server = viser.ViserServer(share=True)

    H, W = images[0].shape[:2]

    for i, (pose_data, img) in enumerate(zip(poses, images)):
        c2w = pose_data['c2w']
        server.scene.add_camera_frustum(
            f"/cameras/{i}",
            fov=2 * np.arctan2(H / 2, mtx[0, 0]),
            aspect=W / H,
            scale=0.02,
            wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz,
            position=c2w[:3, 3],
            image=img
        )

    try:
        while True:
            time.sleep(0.1)</code></pre>
                    </div>
                </div>

                <h3>Results</h3>

                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 1.5rem 0;">
                    <figure>
                        <img src="src/img/0-3-viser-demo1.jpg" alt="Camera Frustums View 1" class="clickable" style="height: 350px; object-fit: cover;">
                        <figcaption>▲ 3D visualization of camera frustums (view 1)</figcaption>
                    </figure>
                    <figure>
                        <img src="src/img/0-3-viser-demo2.jpg" alt="Camera Frustums View 2" class="clickable" style="height: 350px; object-fit: cover;">
                        <figcaption>▲ 3D visualization of camera frustums (view 2)</figcaption>
                    </figure>
                </div>
            </div>
        </section>

        <section id="part0-4" class="info-section">
            <div class="content-centered">
                <h2>Part 0.4: Dataset Preparation</h2>

                <p>The final step is to <strong>undistort all images</strong> and package everything into a dataset format ready for NeRF training. This combines all previous steps from Part 0.1 to 0.3.</p>

                <h3>Implementation</h3>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> Dataset Creation Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python">def create_nerf_dataset(img_dir, mtx, dist, tag_size=0.06,
                        train_ratio=0.7, val_ratio=0.15,
                        output_path='output/my_data.npz'):
    aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    aruco_params = cv2.aruco.DetectorParameters()
    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)

    tag_3d = np.array([
        [0, 0, 0],
        [tag_size, 0, 0],
        [tag_size, tag_size, 0],
        [0, tag_size, 0]
    ], dtype=np.float32)

    img_files = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))
    all_images = []
    all_c2ws = []

    for img_path in img_files:
        img = cv2.imread(img_path)
        if img is None:
            continue

        corners, ids, _ = detector.detectMarkers(img)

        if ids is not None and len(ids) > 0:
            img_points = corners[0].reshape(-1, 2)
            success, rvec, tvec = cv2.solvePnP(tag_3d, img_points, mtx, dist)

            if success:
                # Key steps: undistort and convert color
                undistorted = cv2.undistort(img, mtx, dist)
                undistorted_rgb = cv2.cvtColor(undistorted, cv2.COLOR_BGR2RGB)

                # Compute c2w matrix
                R, _ = cv2.Rodrigues(rvec)
                w2c = np.eye(4)
                w2c[:3, :3] = R
                w2c[:3, 3] = tvec.flatten()
                c2w = np.linalg.inv(w2c)

                all_images.append(undistorted_rgb)
                all_c2ws.append(c2w)

    all_images = np.array(all_images)
    all_c2ws = np.array(all_c2ws)

    # Split into train/val/test
    n_total = len(all_images)
    n_train = int(n_total * train_ratio)
    n_val = int(n_total * val_ratio)

    idx = np.random.permutation(n_total)
    train_idx = idx[:n_train]
    val_idx = idx[n_train:n_train + n_val]
    test_idx = idx[n_train + n_val:]

    focal = mtx[0, 0]

    # Save as .npz
    np.savez(
        output_path,
        images_train=all_images[train_idx],
        c2ws_train=all_c2ws[train_idx],
        images_val=all_images[val_idx],
        c2ws_val=all_c2ws[val_idx],
        c2ws_test=all_c2ws[test_idx],
        focal=focal
    )

    return (all_images[train_idx], all_c2ws[train_idx],
            all_images[val_idx], all_c2ws[val_idx],
            all_c2ws[test_idx], focal)</code></pre>
                    </div>
                </div>
                <h3>Dataset Structure</h3>

                <div class="result-box">
                    <h4>Output: my_data.npz</h4>
                    <p>The packaged dataset contains:</p>
                    <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
                        <li><code class="code-inline">images_train</code>: Training images (undistorted RGB)</li>
                        <li><code class="code-inline">c2ws_train</code>: Camera-to-world matrices for training (0.7)</li>
                        <li><code class="code-inline">images_val</code>: Validation images</li>
                        <li><code class="code-inline">c2ws_val</code>: Camera-to-world matrices for validation (0.15)</li>
                        <li><code class="code-inline">c2ws_test</code>: Camera-to-world matrices for test (0.15)</li>
                        <li><code class="code-inline">focal</code>: Focal length (from intrinsic matrix)</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- PART 1: NEURAL FIELD FOR 2D IMAGES -->

        <section id="part1" class="info-section">
            <div class="content-centered">
                <h2>Part 1: Neural Field for 2D Images</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <!-- PART 2: NEURAL RADIANCE FIELD (3D) -->

        <section id="part2-1" class="info-section">
            <div class="content-centered">
                <h2>Part 2.1: Ray Generation</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-2" class="info-section">
            <div class="content-centered">
                <h2>Part 2.2: Point Sampling</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-3" class="info-section">
            <div class="content-centered">
                <h2>Part 2.3: Data Loading</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-4" class="info-section">
            <div class="content-centered">
                <h2>Part 2.4: NeRF Network</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-5" class="info-section">
            <div class="content-centered">
                <h2>Part 2.5: Volume Rendering</h2>
                <p>[Your content here]</p>
            </div>
        </section>

        <section id="part2-6" class="info-section">
            <div class="content-centered">
                <h2>Part 2.6: Training with Custom Data</h2>
                <p>[Your content here]</p>
            </div>
        </section>

    </main>

    <!-- Lightbox -->
    <div id="lightbox" class="lightbox">
        <img id="lightbox-img">
        <span class="close">&times;</span>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        // Toggle code blocks
        function toggleCode(element) {
            const codeContent = element.nextElementSibling;
            const toggleIcon = element.querySelector('.toggle-icon');
            if (codeContent.style.display === 'none') {
                codeContent.style.display = 'block';
                toggleIcon.textContent = '▼';
            } else {
                codeContent.style.display = 'none';
                toggleIcon.textContent = '▶';
            }
        }

        // Image lightbox
        document.querySelectorAll('.clickable').forEach(img => {
            img.addEventListener('click', function() {
                const lightbox = document.getElementById('lightbox');
                const lightboxImg = document.getElementById('lightbox-img');
                lightbox.style.display = 'flex';
                lightboxImg.src = this.src;
            });
        });

        document.querySelector('.lightbox .close').addEventListener('click', function() {
            document.getElementById('lightbox').style.display = 'none';
        });

        document.getElementById('lightbox').addEventListener('click', function(e) {
            if (e.target === this) {
                this.style.display = 'none';
            }
        });

        // Smooth scroll
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>

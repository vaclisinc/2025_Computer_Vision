<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 2: Fun with Filters and Frequencies</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <!-- Header -->
    <header>
        <div class="header-content">
            <div class="header-info">
                <a href="../index.html" class="back-link">← Back to Projects</a>
                <h1>Fun with Filters and Frequencies</h1>
                <p class="subtitle">CS 180 Project 2 · Song-Ze Yu</p>
            </div>
        </div>
    </header>

    <!-- Navigation -->
    <nav class="toc">
        <a href="#part1-1">1.1 Convolutions</a>
        <a href="#part1-2">1.2 Edge Detection</a>
        <a href="#part1-3">1.3 DoG Filters</a>
        <a href="#part2-1">2.1 Sharpening</a>
        <a href="#part2-2">2.2 Hybrid Images</a>
        <a href="#part2-3">2.3 Stacks</a>
        <a href="#part2-4">2.4 Blending</a>
    </nav>

    <main>
        <!-- Part 1.1: Convolutions -->
        <section id="part1-1">
            <h2>Part 1.1: Convolutions from Scratch</h2>

            <p>I implemented 2D convolution three ways to understand performance trade-offs:</p>

            <!-- Code Implementation -->
            <h3>1.1.1 Three Convolution Implementations</h3>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Implementation 1: Naive (4 loops)
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def convolve_naive(image, kernel):
    h, w = image.shape
    kh, kw = kernel.shape

    # Padding
    pad_h = kh // 2
    pad_w = kw // 2
    padded = np.pad(image,
        ((pad_h, pad_h), (pad_w, pad_w)),
        mode='constant')
    output = np.zeros((h, w))

    for i in range(h):
        for j in range(w):
            for ki in range(kh):
                for kj in range(kw):
                    # Flip kernel
                    fi = kh - 1 - ki
                    fj = kw - 1 - kj
                    output[i,j] += padded[i+ki, j+kj] * kernel[fi, fj]
    return output</code></pre>
                </div>
            </div>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Implementation 2: Optimized (2 loops)
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def convolve_optimized(image, kernel):
    h, w = image.shape
    kh, kw = kernel.shape

    # Padding
    pad_h = kh // 2
    pad_w = kw // 2
    padded = np.pad(image,
        ((pad_h, pad_h), (pad_w, pad_w)),
        mode='constant')

    output = np.zeros((h, w))

    # Pre-flip kernel
    flipped = np.flip(np.flip(kernel, 0), 1)

    for i in range(h):
        for j in range(w):
            region = padded[i:i+kh, j:j+kw]
            output[i,j] = np.sum(region * flipped)

    return output</code></pre>
                </div>
            </div>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Implementation 3: Using SciPy
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def convolve_scipy(image, kernel):
    output = scipy.signal.convolve2d(
        image, kernel,
        mode='same',
        boundary='fill',
        fillvalue=0)
    return output</code></pre>
                </div>
            </div>

            <!-- Box Filter Test -->
            <h3>1.1.2 Testing with 9×9 Box Filter</h3>
            <!-- <p>To test our implementations, I used a 9×9 box filter:</p> -->

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Creating a Box Filter
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def create_box_filter(size):
    return np.ones((size, size)) / (size * size)

box_filter = create_box_filter(9)</code></pre>
                </div>
            </div>

            <!-- Performance Results with Image Side by Side -->
            <div class="results-with-image">
                <div class="left-content">
                    <div class="performance-table">
                        <table>
                            <tr>
                                <th>Implementation</th>
                                <th>9×9 Box Filter</th>
                                <th>Speedup</th>
                                <th>Max Error</th>
                            </tr>
                            <tr>
                                <td>Naive</td>
                                <td class="time-slow">32.65s</td>
                                <td>1×</td>
                                <td rowspan="3">< 10⁻¹⁵</td>
                            </tr>
                            <tr>
                                <td>Optimized</td>
                                <td class="time-med">3.03s</td>
                                <td>10.8×</td>
                            </tr>
                            <tr>
                                <td>Scipy</td>
                                <td class="time-fast">0.11s</td>
                                <td>295×</td>
                            </tr>
                        </table>
                    </div>
                    <div class="accuracy-details">
                        <h4>Accuracy Verification</h4>
                        <p><strong>Naive vs Scipy max difference:</strong> 2.44 × 10⁻¹⁵</p>
                        <p><strong>Optimized vs Scipy max difference:</strong> 7.77 × 10⁻¹⁶</p>
                        <!-- <p class="note">Differences are within floating-point precision, confirming all implementations are correct.</p> -->
                    </div>
                </div>
                <div class="right-content">
                    <figure>
                        <img src="src/img/part1_1_box_filter_results.png" alt="Box Filter Results" class="clickable">
                        <figcaption>▲ Original (top left) → Blurred with 9×9 box filter (other three show identical results from all implementations)</figcaption>
                    </figure>
                </div>
            </div>

            <!-- Finite Difference Operators -->
            <h3>1.1.3 Finite Difference Operators</h3>
            <p>Next, I tested with finite difference operators to detect edges:</p>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Finite Difference Operators
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Detects vertical edges (horizontal gradient)
Dx = np.array([[-1, 0, 1]])

# Detects horizontal edges (vertical gradient)
Dy = np.array([[-1], [0], [1]])</code></pre>
                </div>
            </div>

            <!-- Finite Difference Results with Table and Images -->
            <div class="results-with-image">
                <div class="left-content">
                    <div class="performance-table">
                        <table>
                            <tr>
                                <th>Implementation</th>
                                <th>Dx Runtime</th>
                                <th>Dy Runtime</th>
                            </tr>
                            <tr>
                                <td>Naive</td>
                                <td class="time-med">1.47s</td>
                                <td class="time-med">1.60s</td>
                            </tr>
                            <tr>
                                <td>Optimized</td>
                                <td class="time-slow">2.89s</td>
                                <td class="time-slow">2.92s</td>
                            </tr>
                            <tr>
                                <td>Scipy</td>
                                <td class="time-fast">0.01s</td>
                                <td class="time-fast">0.02s</td>
                            </tr>
                        </table>
                    </div>
                    <div class="accuracy-details">
                        <h4>Key Observations</h4>
                        <p>• Small kernels (3×3) show naive faster than optimized due to NumPy overhead.</p>
                        <p>• All implementations produce identical results.</p>
                        <p>• <strong>Dx, Dy outputs:</strong> Can be negative or positive, so black = negative, gray = zero, white = positive.</p>
                        <p>• <strong>Gradient Magnitude:</strong> Always positive, so black = 0 (no edge), white = strong edge.</p>
                    </div>
                </div>
                <div class="right-content">
                    <div class="stacked-images">
                        <figure>
                            <img src="src/img/part1_1_Dx_filter_results.png" alt="Dx Filter" class="clickable">
                            <figcaption>▲ <strong>Dx Result:</strong> Detects vertical edges.</figcaption>
                        </figure>
                        <figure>
                            <img src="src/img/part1_1_Dy_filter_results.png" alt="Dy Filter" class="clickable">
                            <figcaption>▲ <strong>Dy Result:</strong> Detects horizontal edges.</figcaption>
                        </figure>
                        <figure>
                            <img src="src/img/part1_1_Dx_Dy_Gradient Magnitude_results.png" alt="Gradient Magnitude" class="clickable">
                            <figcaption>▲ <strong>Gradient Magnitude:</strong> √(Dx² + Dy²) </figcaption>
                        </figure>
                    </div>
                </div>
            </div>
<!-- 
            <div class="observation-box">
                <h4>Key Observation: Color Representation</h4>
                <p><strong>Dx and Dy outputs:</strong> Can be negative or positive, so zero appears as gray (middle value), with black showing negative gradients and white showing positive gradients.</p>
                <p><strong>Gradient Magnitude:</strong> Always positive (due to squaring), so zero appears as pure black, with brighter values indicating stronger edges regardless of direction.</p>
            </div> -->
        </section>

        <!-- Part 1.2: Edge Detection -->
        <section id="part1-2">
            <h2>Part 1.2: Finite Difference Operator</h2>

            <p>Edge detection via gradient magnitude and thresholding. After learning how to implement convolution from scratch in Part 1.1, we now use scipy for efficiency.</p>

            <!-- Initial Gradient -->
            <h3>1.2.1 Applying Finite Difference to Cameraman</h3>
            <p> Here we apply the same Dx and Dy operators to the classic cameraman.png image and compute the gradient magnitude.</p>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Image Preprocessing
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Important: The cameraman.png downloaded from the project page
# is not a 2D grayscale image, so we need to convert it

if len(cameraman.shape) == 3:
    if cameraman.shape[2] == 4:  # RGBA image
        cameraman = sk.color.rgba2rgb(cameraman)
    cameraman = sk.color.rgb2gray(cameraman)

cameraman = cameraman.astype(np.float64)</code></pre>
                </div>
            </div>

            <div class="image-grid">
                <figure>
                    <img src="src/img/part1_2_partial_derivatives.png" alt="Partial Derivatives" class="clickable">
                    <!-- <figcaption>Partial Derivative in X (Dx), Partial Derivative in Y (Dy), Gradient Magnitude</figcaption> -->
                </figure>
            </div>

            <p>Notice that the contrast is quite poor - the edges appear faint rather than bright white. To address this issue, we need to normalize the gradient magnitude to the range [0,1] and find an optimal threshold to filter out noise -> this is called <b>binary edge detection.</b></p>

            <!-- Normalization -->
            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Gradient Normalization
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Normalize gradient magnitude to [0, 1] range
normalized = gradient_magnitude / np.max(gradient_magnitude)</code></pre>
                </div>
            </div>

            <!-- Threshold Search -->
            <h3>1.2.2 Finding Optimal Threshold</h3>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Coarse Search
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Coarse search to find approximate optimal threshold
thresholds = [0.01, 0.05, 0.10, 0.15, 0.20, 0.25]
binary_edges = []

for threshold in thresholds:
    binary_edge = (normalized_gradient_cameraman > threshold).astype(np.float64)
    binary_edges.append(binary_edge)</code></pre>
                </div>
            </div>

            <figure>
                <img src="src/img/part1_2_threshold_comparison_0.01_to_0.25.png" alt="Coarse Search" class="clickable">
                <figcaption>▲ Coarse search: 0.01 (too noisy) → 0.25 (too sparse)</figcaption>
            </figure>

            <div class="code-block collapsible"  style="margin-top: 20px">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Fine-tuned Search
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Fine-tuned search around the promising range
thresholds = [0.19, 0.20, 0.21, 0.22, 0.23]
binary_edges = []

for threshold in thresholds:
    binary_edge = (normalized_gradient_cameraman > threshold).astype(np.float64)
    binary_edges.append(binary_edge)</code></pre>
                </div>
            </div>

            <figure>
                <img src="src/img/part1_2_threshold_comparison_0.19_to_0.23.png" alt="Fine Search" class="clickable">
                <figcaption>▲ Fine-tuning: 0.19-0.23, optimal at 0.21</figcaption>
            </figure>

            <!-- Final Result with Trade-off -->
            <div class="highlight-result-with-tradeoff">
                <img src="src/img/part1_2_edge_threshold_0.21_final.png" alt="Final Edge Detection" class="clickable">
                <div class="right-panel">
                    <div class="result-info">
                        <h4>Optimal: T = 0.21</h4>
                        <p>It's a difficult trade-off: if the threshold is higher, it reduces noise but also blurs the real edges of the cameraman.</p>
                        <p>Since no matter how large the threshold is, the bridge remains as high-frequency as the cameraman, it's impossible to isolate only the cameraman.</p>
                        <p>Hence, this setting is already the best balance between noise suppression and edge preservation.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Part 1.3: DoG Filters -->
        <section id="part1-3">
            <h2>Part 1.3: Derivative of Gaussian (DoG) Filter</h2>

            <p>The edges from Part 1.2 appear noisy. We can improve edge detection by first smoothing the image with a Gaussian filter before computing gradients.</p>
                    <div class="code-block collapsible">
                        <h4 class="code-toggle" onclick="toggleCode(this)">
                            <span class="toggle-icon">▶</span>  Gaussian Kernel Creation
                        </h4>
                        <div class="code-content" style="display: none;">
                            <pre><code class="language-python"># Since a 2D Gaussian is separable:
# G(x,y) = e^(-(x²+y²)/2σ²) = e^(-x²/2σ²) · e^(-y²/2σ²)
#
# So the 2D kernel is just the outer product of the 1D Gaussian with itself.

def create_gaussian_kernel(ksize, sigma):
    gaussian_1d = cv2.getGaussianKernel(ksize, sigma)  # Get 1D Gaussian
    gaussian_2d = gaussian_1d @ gaussian_1d.T           # Outer product
    return gaussian_2d</code></pre>
                        </div>
                    </div>

            <div class="observation-box" style="margin-top: 1rem;">
                <h4>Understanding σ (Sigma)</h4>
                <p><strong>σ</strong> is the standard deviation that controls the blur intensity:</p>
                <ul>
                    <li><strong>Small σ</strong> → Sharp, blurs only a small neighborhood</li>
                    <li><strong>Large σ</strong> → Strong blur, affects a wider range</li>
                </ul>
            </div>

            <!-- Two-Step Approach -->
            <div style="margin: 30px 0;">
                <h3>1.3.1 Two-Step Approach: Blur Then Gradient</h3>
                    <p>First, we apply a Gaussian filter to smooth the image, then compute gradients:</p>

                    <div class="code-block collapsible">
                        <h4 class="code-toggle" onclick="toggleCode(this)">
                            <span class="toggle-icon">▶</span> Two-Step Implementation
                        </h4>
                        <div class="code-content" style="display: none;">
                            <pre><code class="language-python"># Step 1: Apply Gaussian blur
gaussian_kernel = create_gaussian_kernel(7, 1)
blurred_cameraman = scipy.signal.convolve2d(
    cameraman, gaussian_kernel,
    mode='same', boundary='symm'
)

# Step 2: Apply derivative operators
Dx_blurred = scipy.signal.convolve2d(
    blurred_cameraman, Dx,
    mode='same', boundary='symm'
)
Dy_blurred = scipy.signal.convolve2d(
    blurred_cameraman, Dy,
    mode='same', boundary='symm'
)

# Compute gradient magnitude
gradient_magnitude_blurred = np.sqrt(Dx_blurred**2 + Dy_blurred**2)</code></pre>
                        </div>
                    </div>
<div class="observation-box">
                        <h4>Why Choose kernel_size=7, sigma=1?</h4>
                        <p>A common choice is to choose <strong>k = 6σ + 1</strong></p>
                        <ul>
                            <li>Gaussian kernel contains most (>99%) of the useful weights</li>
                            <li>Kernel size too tiny: cut off the Gaussian</li>
                            <li>Kernel size too large: waste compute since outside the ±3σ weights are very close to 0</li>
                        </ul>
                    </div>
                <figure style="margin-top: 20px;">
                    <img src="src/img/part1_3_two_step_partial_derivatives.png" alt="Two-Step" class="clickable" style="width: 100%;">
                    <figcaption>▲ Two-step: Gaussian smoothing → Gradient</figcaption>
                </figure>
            </div>

            <!-- One-Step Approach -->
            <div style="margin: 30px 0;">
                <h3 style="border-top: 1px solid #e5e5e5; padding-top: 2rem; margin-top: 3rem;">1.3.2 One-Step Approach: DoG Filters</h3>
                    <p>Due to convolution associativity, we can combine the Gaussian and derivative into a single DoG filter:</p>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> DoG Filter Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python"># DoG filters: Convolve Gaussian with derivative operators (Dx, Dy) using 'full' mode 
# since we want to keep the entire convolution result, including boundary effects → output is larger than the input.

def create_DoG_kernel(gaussian_kernel, derivative_operators):
    return scipy.signal.convolve2d(gaussian_kernel, derivative_operators, mode='full')

DoG_x = create_DoG_kernel(gaussian_kernel, Dx)
DoG_y = create_DoG_kernel(gaussian_kernel, Dy)
# This is mathematically equivalent but computationally more efficient</code></pre>
                    </div>
                </div>
                <figure style="margin-top: 20px; text-align: center;">
                    <img src="src/img/part1_3_dog_filters_visualization.png" alt="DoG Filter Visualization" class="clickable" style="width: 50%;">
                    <figcaption>▲ DoG Filter Visualization</figcaption>
                </figure>
                <div class="code-block collapsible" style="margin-top: 20px">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> One-Step Implementation: Convolve with DoG
                    </h4>
                <div class="code-content" style="display: none;">
                        <pre><code class="language-python">DoG_x_cameraman = scipy.signal.convolve2d(cameraman, DoG_x, mode='same', boundary='symm')
DoG_y_cameraman = scipy.signal.convolve2d(cameraman, DoG_y, mode='same', boundary='symm')
gradient_magnitude_DoG_cameraman = np.sqrt(DoG_x_cameraman ** 2 + DoG_y_cameraman ** 2)</code></pre>
                    </div>
                </div>
                

                <figure style="margin-top: 20px;">
                    <img src="src/img/part1_3_one_step_partial_derivatives.png" alt="One-Step" class="clickable" style="width: 100%;">
                    <figcaption>▲ One-step DoG approach (identical results)</figcaption>
                </figure>

                <div class="observation-box" style="margin-top: 20px;">
                    <h4>1.3.1 & 1.3.2 Verification: Two-Step vs One-Step</h4>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; align-items: center;">
                        <figure style="margin: 0;">
                            <img src="src/img/part1_3_twostep_vs_dog_comparison.png" alt="Two-Step vs DoG Comparison" class="clickable" style="width: 100%;">
                        </figure>
                        <div>
                            <p><strong>Numerical Verification:</strong></p>
                            <ul style="margin-top: 0.5rem;">
                                <li>Max difference in D<sub>x</sub>: <strong>0.0000000000</strong></li>
                                <li>Max difference in D<sub>y</sub>: <strong>0.0000000000</strong></li>
                                <li>Max difference in gradient magnitude: <strong>0.0000000000</strong></li>
                            </ul>
                            <p style="margin-top: 1rem;">The results are mathematically identical, confirming the associative property of convolution: (G * I) * D = I * (G * D)</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="implementation-note" style="margin-top: 2rem;">
                <h4>[Note.] Implementation Detail: Convolution Padding</h4>
                <p>When implementing convolution, using <code>boundary='fill'</code> will produce incorrect results, but <code>boundary='symm'</code> gives the correct output. Here's why:</p>
                <p><strong>Why <code>boundary='fill'</code> fails:</strong></p>
                <ul>
                    <li>Our kernel size is relatively small (e.g., DoG filter visualization shows edges are still gray, not zero)</li>
                    <li>When boundary values are treated as 0 (black), a <strong>"dark edge effect"</strong> appears at borders</li>
                    <li>The convolution kernel near edges mixes in these 0 values → causes brightness reduction and obvious dark frames</li>
                </ul>
                <p><strong>Why <code>boundary='symm'</code> (mirror padding) works:</strong></p>
                <ul>
                    <li>Boundary values are <strong>mirrored</strong> symmetrically, like looking in a mirror</li>
                    <li>The kernel at edges can still get <strong>reasonable values</strong> instead of encountering zeros</li>
                    <li>The effect is more natural, without dark edges or sudden brightness changes</li>
                    <li>Most image processing libraries (OpenCV, Pillow, MATLAB) use mirror/reflection padding by default for sharpening and blurring operations</li>
                </ul>
            </div>

            <h3 style="border-top: 1px solid #e5e5e5; padding-top: 2rem; margin-top: 3rem;">1.3.3 Comparing Edge Detection: With vs Without Blur</h3>
            <p>We can find out that with DoG Filter, the edge is clearer since it <strong>removes high-frequency noise through Gaussian</strong> smoothing while preserving the main edge structures.</p>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                <!-- Left Column: Without Gaussian Blur -->
                <div>
                    <h4 style="text-align: center; margin-bottom: 1rem;">(1.2) Without Gaussian Blur</h4>
                    <figure style="margin: 0 0 1.5rem 0;">
                        <img src="src/img/part1_3_no_blur_partial_derivatives.png" alt="No Blur Derivatives" class="clickable" style="width: 100%;">
                        <figcaption>▲ Partial derivatives (noisy)</figcaption>
                    </figure>
                    <figure style="margin: 0;text-align: center;">
                        <img src="src/img/part1_2_edge_threshold_0.21_final.png" alt="No Blur Edge Detection" class="clickable" style="width: 70%;">
                        <figcaption>▲ Binary edge detection (T=0.21)</figcaption>
                    </figure>
                </div>

                <!-- Right Column: With DoG (Gaussian Blur) -->
                <div>
                    <h4 style="text-align: center; margin-bottom: 1rem;">(1.3) With DoG Filter (Gaussian Blur)</h4>
                    <figure style="margin: 0 0 1.5rem 0;">
                        <img src="src/img/part1_3_one_step_partial_derivatives.png" alt="DoG Derivatives" class="clickable" style="width: 100%;">
                        <figcaption>▲ Partial derivatives (smoothed)</figcaption>
                    </figure>
                    <figure style="margin: 0;text-align: center;">
                        <img src="src/img/part1_3_edge_threshold_0.21_final.png" alt="DoG Edge Detection" class="clickable" style="width: 70%;">
                        <figcaption>▲ Binary edge detection (T=0.21)</figcaption>
                    </figure>
                </div>
            </div>


            <h3>1.3.4 Optimal Threshold with DoG</h3>
            <p>After applying the DoG filter, the high-frequency noise has been removed, which changes the gradient magnitude distribution. Therefore, we need to find a new optimal threshold for edge detection:</p>

            <div class="image-grid">
                <figure>
                    <img src="src/img/part1_3_threshold_comparison_0.01_to_0.50.png" alt="DoG Coarse Search" class="clickable">
                    <figcaption>▲ Coarse search: 0.01 → 0.50</figcaption>
                </figure>
                <figure>
                    <img src="src/img/part1_3_threshold_comparison_0.35_to_0.5.png" alt="DoG Fine Search" class="clickable">
                    <figcaption>▲ Fine-tuning: 0.35 → 0.50</figcaption>
                </figure>
            </div>
            <div class="highlight-result-with-tradeoff" style="grid-template-columns: 1.5fr 1fr;">
                <img src="src/img/part1_3_edge_comparison_threshold_0.41.png" alt="Comparison" class="clickable" style="width: 100%;">
                <div class="right-panel">
                    <div class="result-info">
                        <h4>Optimal: T = 0.41</h4>
                        <p>• DoG allows using higher thresholds (0.41 vs 0.21) while preserving important edges</p>
                        <p>• Without DoG, at threshold 0.41 the edges have already lost half of the details</p>
                    </div>
                </div>
            </div>

        </section>

        <!-- Part 2.1: Sharpening -->
        <section id="part2-1">
            <h2>Part 2.1: Image Sharpening with Unsharp Masking</h2>

            <p>Unsharp masking sharpens images by first blurring (applying Gaussian filter), then subtracting to get high frequencies (similar to Laplacian), and finally adding them back to enhance edges.</p>

            <h3>2.1.1 Step-by-Step Implementation</h3>
            <p>Let's break down the sharpening process into individual steps:</p>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Stepwise Unsharp Masking
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def apply_unsharp_mask_stepwise_demo(image, kernel_size=9, sigma=2.0, alpha=1.5):
    gaussian_kernel = create_gaussian_kernel(kernel_size, sigma)

    # Step 1: Blur the image (low-pass filter)
    blurred = np.zeros_like(image)
    for i in range(3):
        blurred[:,:,i] = scipy.signal.convolve2d(
            image[:,:,i], gaussian_kernel, mode='same', boundary='symm'
        )

    # Step 2: Extract high frequencies
    high_freq = image - blurred

    # Step 3: Add amplified high frequencies back
    sharpened = image + alpha * high_freq
    sharpened = np.clip(sharpened, 0, 1)

    return blurred, high_freq, sharpened</code></pre>
                </div>
            </div>

            <figure style="margin: 2rem 0;">
                <img src="src/img/part2_1_taj_sharpen_demo.jpg" alt="Taj Mahal Sharpening Process" class="clickable" style="width: 100%;">
                <figcaption>▲ Sharpening pipeline: Original → Blurred (low freq) → High freq → Sharpened (α=1)</figcaption>
            </figure>

            <h3>2.1.2 Simplified: Single Unsharp Mask Kernel</h3>
            <p>We don't need to perform these steps separately. We can combine them into a single <strong>unsharp mask kernel</strong> and apply it in one convolution:</p>
            <div class="observation-box">
                <h4>Unsharp Masking Formula Derivation</h4>
                <div style="text-align: left; margin: 1rem 0; font-size: 1rem; font-family: 'JetBrains Mono', monospace; line-height: 2;">
                    <strong>sharpened</strong> = original + α × (original - blurred)<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = (1 + α) × original - α × blurred<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = (1 + α) × original - α × (gaussian ⊗ original)<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = <strong>[(1 + α) × identity - α × gaussian] ⊗ original</strong>
                </div>
                <p style="margin-top: 1rem;">Therefore, the <strong>unsharp kernel</strong> = (1 + α) × identity - α × gaussian</p>
                <p>Where <strong>α</strong> controls the sharpening strength. Higher α = more pronounced edges.</p>
                <p><em>Note: "identity" here refers to the identity convolution kernel (center=1, rest=0), not the linear algebra identity matrix.</em></p>
            </div>
            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Unsharp Mask Kernel Creation
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def create_unsharp_mask_kernel(kernel_size, sigma, alpha):
    gaussian = create_gaussian_kernel(kernel_size, sigma)        # low pass
    identity = np.zeros_like(gaussian)
    center = kernel_size // 2
    identity[center, center] = 1
    unsharp_kernel = (1 + alpha) * identity - alpha * gaussian   # I + alpha * (I - low pass) => sharper
    return unsharp_kernel</code></pre>
                </div>
            </div>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Convolve with Unsharp Mask Kernel
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def apply_unsharp_mask(image, kernel_size=9, sigma=2.0, alpha=1.5):
    unsharp_kernel = create_unsharp_mask_kernel(kernel_size, sigma, alpha)

    result = np.zeros_like(image)
    for i in range(3):
        result[:,:,i] = scipy.signal.convolve2d(
            image[:,:,i], unsharp_kernel, mode='same', boundary='symm'
        )

    return np.clip(result, 0, 1)  # restrict to [0, 1]: <0 -> black; >1: white</code></pre>
                </div>
            </div>

            <h3>2.1.3 Testing Different Alpha Values</h3>
            <p>The α parameter controls sharpening intensity. Let's compare different values:</p>

            <figure style="margin: 2rem 0;">
                <img src="src/img/part2_1_taj_alpha_comparison.png" alt="Taj Alpha Comparison" class="clickable" style="width: 100%;">
                <figcaption>▲ Effect of α on sharpening: α=0.5, 1.0, 2.0, 3.0</figcaption>
            </figure>

            <h3>2.1.4 Blur-Sharpen-Blur-Sharpen Experiment</h3>
            <p>What happens if we blur a sharpened image and sharpen it again?</p>

            <figure style="margin: 2rem 0;">
                <img src="src/img/part2_1_taj_sharpen_blurred_sharpen.jpg" alt="Blur-Sharpen Cycle" class="clickable" style="width: 100%;">
                <figcaption>▲ Original → Sharpened → Blurred → Re-sharpened</figcaption>
            </figure>

            <div class="observation-box">
                <p><strong>Lesson:</strong> Information loss in blurring is irreversible</p>
                <ul>
                    <li>Some high-frequency details are <strong>permanently lost</strong> in the blur step</li>
                    <li>The result is softer than the original sharpened version</li>
                    <li>We <strong>partially recover</strong> sharpness, but not fully</li>
                </ul>
            </div>

            <h3>2.1.5 Custom Image: Shaved Snow</h3>
            <p>This image holds special meaning for me — it was taken with friends a week before I came to the US for exchange. The day before taking this photo, I accidentally fell asleep in the bathtub while showering, and my phone got water damaged. Can CS180 techniques rescue this photo?</p>

            <p>At first, I loaded the picture as-is, but its shape is <code class="code-inline">(4032, 2268, 3)</code> — way bigger than the Taj image. The default <code class="code-inline">kernel_size=9</code> is too small to capture many edge details at this resolution. To be honest, I can't really tell the difference:</p>

            <figure style="margin: 2rem 0; text-align: center;">
                <img src="src/img/part2_1_shaved_snow_alpha_comparison.png" alt="Shaved Snow Alpha Comparison" class="clickable" style="width: 75%;">
                <figcaption>▲ Sharpening on original resolution (α values: 0.5, 1.0, 2.0, 3.0) — barely visible differences</figcaption>
            </figure>

            <p>My solution: resize the image using <code class="code-inline">transform.resize(shaved_snow, (600, 337), anti_aliasing=True)</code>. Now we can actually see the results:</p>

            <figure style="margin: 2rem 0;  text-align: center;">
                <img src="src/img/part2_1_shaved_snow_resized_alpha_comparison.png" alt="Resized Shaved Snow Comparison" class="clickable" style="width: 75%;">
                <figcaption>▲ Sharpening on resized image (600×337) — now the differences are visible!</figcaption>
            </figure>

            <div class="observation-box">
                <h4>The photo wasn't fully rescued:</h4>
                <ul>
                    <li>While sharpening helps, it can't fully recover lost details from the water damage.</li>
                    <li>No matter how advanced image processing techniques are, the most important thing is still the <strong>quality of the original signal</strong></li>
                    <li><strong>Kernel size matters:</strong> Must be proportional to image resolution for effective sharpening</li>
                </ul>
            </div>

        </section>

        <!-- Part 2.2: Hybrid Images -->
        <section id="part2-2">
            <h2>Part 2.2: Hybrid Images</h2>

            <p>High-frequency details dominate at close distances, vice versa. By combining the low frequencies of one image with the high frequencies of another, we create an image that changes appearance based on viewing distance.</p>

            <h3>2.2.1 Image Selection & Originals</h3>
            <p>I created three hybrid image pairs. Here are the original photos before alignment and filtering:</p>

            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 1.5rem; margin: 2rem 0;">
                <!-- Derek + Nutmeg -->
                <div class="observation-box">
                    <h4 style="text-align: center;">Derek + Nutmeg</h4>
                    <!-- <p>Classic example from the project spec.</p> -->
                    <div style="display: flex; flex-direction: column; gap: 0.75rem; margin-top: 1rem;">
                        <figure style="margin: 0; text-align: center;">
                            <img src="src/img/part2_2_DerekPicture.jpg" alt="Derek" class="clickable" style="width: 60%; border-radius: 4px;">
                            <figcaption style="font-size: 0.75rem; margin-top: 0.25rem;">Derek</figcaption>
                        </figure>
                        <figure style="margin: 0; text-align: center;">
                            <img src="src/img/part2_2_nutmeg.jpg" alt="Nutmeg" class="clickable" style="width: 60%; border-radius: 4px;">
                            <figcaption style="font-size: 0.75rem; margin-top: 0.25rem;">Nutmeg</figcaption>
                        </figure>
                    </div>
                </div>

                <!-- Jay Chou + Me -->
                <div class="observation-box">
                    <h4 style="text-align: center;">Jay Chou + Me</h4>
                    <p>When I was a child, I appeared on a TV show imitating Asian singer Jay Chou. Now I can see how close the resemblance was!</p>
                    <div style="display: flex; flex-direction: column; gap: 0.75rem; margin-top: 1rem;">
                        <figure style="margin: 0; text-align: center;">
                            <img src="src/img/part2_2_jaychou.jpg" alt="Jay Chou" class="clickable" style="width: 60%; border-radius: 4px;">
                            <figcaption style="font-size: 0.75rem; margin-top: 0.25rem;">Jay Chou</figcaption>
                        </figure>
                        <figure style="margin: 0; text-align: center;">
                            <img src="src/img/part2_2_me.jpg" alt="Me" class="clickable" style="width: 60%; border-radius: 4px;">
                            <figcaption style="font-size: 0.75rem; margin-top: 0.25rem;">Me</figcaption>
                        </figure>
                    </div>
                </div>

                <!-- Shohei + Wife -->
                <div class="observation-box">
                    <h4 style="text-align: center;">Shohei Ohtani + Wife</h4>
                    <p>I've seen many news articles claiming they have couple faces. Let's see if a hybrid confirms this!</p>
                    <figure style="margin-top: 1rem; text-align: center;">
                        <img src="src/img/part2_2_shohei-shohei-wife.jpg" alt="Shohei and Wife" class="clickable" style="width: 100%; border-radius: 4px;">
                        <figcaption style="font-size: 0.75rem; margin-top: 0.25rem;">Shohei & Wife</figcaption>
                    </figure>
                </div>
            </div>

            <h3>2.2.2 Image Alignment</h3>
            <p>Before creating hybrid images, we need to align the facial features. I used an interactive clicking tool (<code class="code-inline">part2_2_align_image.py</code>) to manually select two corresponding points on both images, which are then used as anchors for rotation and scaling transformations.</p>

            <figure style="margin: 2rem 0;">
                <img src="src/img/part2_2_demo_alignment_tool.jpg" alt="Alignment Tool Demo" class="clickable" style="width: 50%; display: block; margin: 0 auto; border-radius: 8px;">
                <figcaption>▲ Interactive alignment tool: Select two corresponding points (e.g., eyes) on each image for automatic rotation and scaling</figcaption>
            </figure>

            <h3>2.2.3 Frequency Filtering Implementation</h3>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Low-Pass Filter
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def low_pass_filter(image, sigma=2.0):
    kernel_size = int(2 * np.ceil(3 * sigma) + 1)
    gaussian_kernel = create_gaussian_kernel(kernel_size, sigma)

    blurred = np.zeros_like(image)
    for i in range(3):
        blurred[:,:,i] = scipy.signal.convolve2d(
            image[:,:,i], gaussian_kernel, mode='same', boundary='symm'
        )
    return blurred</code></pre>
                </div>
            </div>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> High-Pass Filter
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def high_pass_filter(image, sigma=2.0):
    low_freq = low_pass_filter(image, sigma)
    high_freq = image - low_freq
    return high_freq</code></pre>
                </div>
            </div>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Create Hybrid Image
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def create_hybrid_image(image1, image2, sigma_low, sigma_high, low_weight=1.0, high_weight=1.0):
    low_freq = low_pass_filter(image1, sigma_low)
    high_freq = high_pass_filter(image2, sigma_high)
    hybrid = low_weight * low_freq + high_weight * high_freq
    hybrid = np.clip(hybrid, 0, 1)
    return hybrid, low_freq, high_freq</code></pre>
                </div>
            </div>

            <h3>2.2.4 Derek + Nutmeg</h3>
            <p><strong>Parameters:</strong> σ<sub>low</sub>=10, σ<sub>high</sub>=2</p>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 2rem 0;">
                <figure>
                    <img src="src/img/part2_2_derek_nutmeg.jpg" alt="Derek Nutmeg Hybrid" class="clickable" style="height: 400px; object-fit: contain; width: 100%;">
                    <figcaption>▲ Hybrid progression</figcaption>
                </figure>
                <figure>
                    <img src="src/img/part2_2_derek_nutmeg_fft.png" alt="Derek Nutmeg FFT" class="clickable" style="height: 400px; object-fit: contain; width: 100%;">
                    <figcaption>▲ Fourier analysis</figcaption>
                </figure>
            </div>

            <h3>2.2.5 Jay Chou + Childhood Me</h3>
            <p><strong>Parameters:</strong> σ<sub>low</sub>=10, σ<sub>high</sub>=3</p>

            <figure style="margin: 2rem 0;">
                <img src="src/img/part2_2_jaychou_me.jpg" alt="Jay Chou Me Hybrid" class="clickable" style="width: 60%; display: block; margin: 0 auto;">
                <figcaption>▲ Hybrid result: View from far to see Jay Chou, up close to see childhood me!</figcaption>
            </figure>
            

            <h3>2.2.6 Shohei Ohtani + Wife</h3>
            <p><strong>Parameters:</strong> σ<sub>low</sub>=3, σ<sub>high</sub>=1.3</p>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 2rem 0;">
                <figure>
                    <img src="src/img/part2_2_shohei_and_wife.jpg" alt="Shohei and Wife Hybrid" class="clickable" style="height: 400px; object-fit: contain; width: 100%;">
                    <figcaption>▲ Hybrid showing "couple face"</figcaption>
                </figure>
                <figure>
                    <img src="src/img/part2_2_shohei_wife_fft.png" alt="Shohei Wife FFT" class="clickable" style="height: 400px; object-fit: contain; width: 100%;">
                    <figcaption>▲ Fourier analysis</figcaption>
                </figure>
            </div>

            <div class="observation-box">
                <h4>Observations</h4>
                <ul>
                    <li>The frequency response plots show the separation of low and high frequencies, though <strong>the high-frequency center doesn't appear as dark as expected</strong> (maybe due to visualization scaling?)</li>                </ul>
            </div>

        </section>

        <!-- Part 2.3: Stacks -->
        <section id="part2-3">
            <h2>Part 2.3: Gaussian and Laplacian Stacks</h2>

            <p>Unlike <strong>Gaussian pyramids</strong> which downsample at each level (as seen in Project 1), <strong>Gaussian stacks</strong> keep all levels at the same resolution as the original image. This makes them ideal for multi-resolution blending.</p>

            <div class="observation-box" style="margin: 1.5rem 0;">
                <h4>Stack vs Pyramid</h4>
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem;">
                    <div>
                        <p><strong>Gaussian Pyramid (Project 1):</strong></p>
                        <ul>
                            <li>Each level is <strong>downsampled</strong> (half size)</li>
                            <li>Used for efficient processing</li>
                            <li>Different resolutions at each level</li>
                        </ul>
                    </div>
                    <div>
                        <p><strong>Gaussian Stack (This Project):</strong></p>
                        <ul>
                            <li>All levels keep <strong>same size</strong> as original</li>
                            <li>No downsampling, just increasing blur</li>
                            <li>Essential for multi-resolution blending</li>
                        </ul>
                    </div>
                </div>
            </div>

            <h3>2.3.1 Building Gaussian Stack</h3>
            <p>At each level, we apply progressively stronger Gaussian blur while maintaining the original image size:</p>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Gaussian Stack Implementation
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def build_gaussian_stack(image, levels=5, sigma_base=1):
    stack = [image]
    current = image.copy()

    for i in range(1, levels):
        # Progressively increase sigma
        sigma = sigma_base * (2 ** i)
        kernel_size = int(2 * np.ceil(3 * sigma) + 1)
        kernel = create_gaussian_kernel(kernel_size, sigma)

        # Apply Gaussian blur (handle RGB channels)
        if len(current.shape) == 3:
            blurred = np.zeros_like(current)
            for c in range(3):
                blurred[:, :, c] = scipy.signal.convolve2d(
                    current[:, :, c], kernel, mode='same', boundary='symm'
                )
        else:
            blurred = scipy.signal.convolve2d(
                current, kernel, mode='same', boundary='symm'
            )

        stack.append(blurred)
        current = blurred

    return stack</code></pre>
                </div>
            </div>

            <h3>2.3.2 Building Laplacian Stack</h3>
            <p>The Laplacian stack captures the difference between consecutive Gaussian levels, representing different frequency bands. Note that the <strong>last layer is directly taken from the Gaussian stack's last layer</strong> (not a difference), as there's no next level to subtract.</p>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Laplacian Stack Implementation
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def build_laplacian_stack(gaussian_stack):
    laplacian_stack = []

    # Compute differences between consecutive levels
    for i in range(len(gaussian_stack) - 1):
        laplacian = gaussian_stack[i] - gaussian_stack[i + 1]
        laplacian_stack.append(laplacian)

    # Last layer: directly use the final Gaussian level
    laplacian_stack.append(gaussian_stack[-1])

    return laplacian_stack</code></pre>
                </div>
            </div>

            <div class="observation-box" style="margin: 1.5rem 0;">
                <h4>Why does the last Laplacian layer = last Gaussian layer?</h4>
                <p>In a Laplacian stack, each layer represents a frequency band extracted as the difference between consecutive Gaussian levels. However, the <strong>last layer</strong> has no next level to subtract from, so we keep the final Gaussian layer as-is. This represents the lowest frequency band (most blurred version) of the image.</p>
            </div>

            <h3>2.3.3 Results: Apple and Orange Stacks</h3>
            <p>I built 6-level Gaussian and Laplacian stacks for both apple and orange images:</p>

            <figure style="margin: 2rem 0; text-align: center;">
                <img src="src/img/part2_3_apple_stacks.png" alt="Apple Stacks" class="clickable" style="width: 60%;">
                <figcaption>▲ Apple: Gaussian stack (top) shows progressive blur, Laplacian stack (bottom) shows frequency bands</figcaption>
            </figure>

            <figure style="margin: 2rem 0;  text-align: center;">
                <img src="src/img/part2_3_orange_stacks.png" alt="Orange Stacks" class="clickable" style="width: 60%;">
                <figcaption>▲ Orange: Gaussian stack (top) shows progressive blur, Laplacian stack (bottom) shows frequency bands</figcaption>
            </figure>

            <div class="observation-box">
                <h4>Observations</h4>
                <ul>
                    <li><strong>Gaussian Stack:</strong> Each level becomes progressively more blurred while maintaining full resolution</li>
                    <li><strong>Laplacian Stack:</strong> Each layer (except the last) captures a specific frequency band, appearing mostly gray with edges highlighted</li>
                    <li><strong>Last Laplacian layer:</strong> Contains the low-frequency content (blurry version), identical to the last Gaussian layer</li>
                    <li>These stacks will be used in Part 2.4 for seamless multi-resolution blending</li>
                </ul>
            </div>

        </section>

        <!-- Part 2.4: Blending -->
        <section id="part2-4">
            <h2>Part 2.4: Multi-resolution Blending</h2>
            <p>Coming soon...</p>
        </section>
    </main>

    <!-- Lightbox -->
    <div id="lightbox" class="lightbox">
        <img id="lightbox-img">
        <span class="close">&times;</span>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        // Toggle code blocks
        function toggleCode(element) {
            const codeContent = element.nextElementSibling;
            const toggleIcon = element.querySelector('.toggle-icon');

            if (codeContent.style.display === 'none') {
                codeContent.style.display = 'block';
                toggleIcon.textContent = '▼';
            } else {
                codeContent.style.display = 'none';
                toggleIcon.textContent = '▶';
            }
        }

        // Image lightbox
        document.querySelectorAll('.clickable').forEach(img => {
            img.addEventListener('click', function() {
                const lightbox = document.getElementById('lightbox');
                const lightboxImg = document.getElementById('lightbox-img');
                lightbox.style.display = 'flex';
                lightboxImg.src = this.src;
            });
        });

        document.querySelector('.lightbox .close').addEventListener('click', function() {
            document.getElementById('lightbox').style.display = 'none';
        });

        document.getElementById('lightbox').addEventListener('click', function(e) {
            if (e.target === this) {
                this.style.display = 'none';
            }
        });

        // Smooth scroll
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>
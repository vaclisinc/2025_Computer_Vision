<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 2: Fun with Filters and Frequencies</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <!-- Header -->
    <header>
        <div class="header-content">
            <div class="header-info">
                <a href="../index.html" class="back-link">← Back to Projects</a>
                <h1>Fun with Filters and Frequencies</h1>
                <p class="subtitle">CS 180 Project 2 · Song-Ze Yu</p>
            </div>
        </div>
    </header>

    <!-- Navigation -->
    <nav class="toc">
        <a href="#part1-1">1.1 Convolutions</a>
        <a href="#part1-2">1.2 Edge Detection</a>
        <a href="#part1-3">1.3 DoG Filters</a>
        <a href="#part2-1">2.1 Sharpening</a>
        <a href="#part2-2">2.2 Hybrid Images</a>
        <a href="#part2-3">2.3 Stacks</a>
        <a href="#part2-4">2.4 Blending</a>
    </nav>

    <main>
        <!-- Part 1.1: Convolutions -->
        <section id="part1-1">
            <h2>Part 1.1: Convolutions from Scratch</h2>

            <p>I implemented 2D convolution three ways to understand performance trade-offs:</p>

            <!-- Code Implementation -->
            <h3>1.1.1 Three Convolution Implementations</h3>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Implementation 1: Naive (4 loops)
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def convolve_naive(image, kernel):
    h, w = image.shape
    kh, kw = kernel.shape

    # Padding
    pad_h = kh // 2
    pad_w = kw // 2
    padded = np.pad(image,
        ((pad_h, pad_h), (pad_w, pad_w)),
        mode='constant')
    output = np.zeros((h, w))

    for i in range(h):
        for j in range(w):
            for ki in range(kh):
                for kj in range(kw):
                    # Flip kernel
                    fi = kh - 1 - ki
                    fj = kw - 1 - kj
                    output[i,j] += padded[i+ki, j+kj] * kernel[fi, fj]
    return output</code></pre>
                </div>
            </div>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Implementation 2: Optimized (2 loops)
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def convolve_optimized(image, kernel):
    h, w = image.shape
    kh, kw = kernel.shape

    # Padding
    pad_h = kh // 2
    pad_w = kw // 2
    padded = np.pad(image,
        ((pad_h, pad_h), (pad_w, pad_w)),
        mode='constant')

    output = np.zeros((h, w))

    # Pre-flip kernel
    flipped = np.flip(np.flip(kernel, 0), 1)

    for i in range(h):
        for j in range(w):
            region = padded[i:i+kh, j:j+kw]
            output[i,j] = np.sum(region * flipped)

    return output</code></pre>
                </div>
            </div>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Implementation 3: Using SciPy
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def convolve_scipy(image, kernel):
    output = scipy.signal.convolve2d(
        image, kernel,
        mode='same',
        boundary='fill',
        fillvalue=0)
    return output</code></pre>
                </div>
            </div>

            <!-- Box Filter Test -->
            <h3>1.1.2 Testing with 9×9 Box Filter</h3>
            <!-- <p>To test our implementations, I used a 9×9 box filter:</p> -->

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Creating a Box Filter
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python">def create_box_filter(size):
    return np.ones((size, size)) / (size * size)

box_filter = create_box_filter(9)</code></pre>
                </div>
            </div>

            <!-- Performance Results with Image Side by Side -->
            <div class="results-with-image">
                <div class="left-content">
                    <div class="performance-table">
                        <table>
                            <tr>
                                <th>Implementation</th>
                                <th>9×9 Box Filter</th>
                                <th>Speedup</th>
                                <th>Max Error</th>
                            </tr>
                            <tr>
                                <td>Naive</td>
                                <td class="time-slow">32.65s</td>
                                <td>1×</td>
                                <td rowspan="3">< 10⁻¹⁵</td>
                            </tr>
                            <tr>
                                <td>Optimized</td>
                                <td class="time-med">3.03s</td>
                                <td>10.8×</td>
                            </tr>
                            <tr>
                                <td>Scipy</td>
                                <td class="time-fast">0.11s</td>
                                <td>295×</td>
                            </tr>
                        </table>
                    </div>
                    <div class="accuracy-details">
                        <h4>Accuracy Verification</h4>
                        <p><strong>Naive vs Scipy max difference:</strong> 2.44 × 10⁻¹⁵</p>
                        <p><strong>Optimized vs Scipy max difference:</strong> 7.77 × 10⁻¹⁶</p>
                        <!-- <p class="note">Differences are within floating-point precision, confirming all implementations are correct.</p> -->
                    </div>
                </div>
                <div class="right-content">
                    <figure>
                        <img src="src/img/part1_1_box_filter_results.png" alt="Box Filter Results" class="clickable">
                        <figcaption>▲ Original (top left) → Blurred with 9×9 box filter (other three show identical results from all implementations)</figcaption>
                    </figure>
                </div>
            </div>

            <!-- Finite Difference Operators -->
            <h3>1.1.3 Finite Difference Operators</h3>
            <p>Next, I tested with finite difference operators to detect edges:</p>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Finite Difference Operators
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Detects vertical edges (horizontal gradient)
Dx = np.array([[-1, 0, 1]])

# Detects horizontal edges (vertical gradient)
Dy = np.array([[-1], [0], [1]])</code></pre>
                </div>
            </div>

            <!-- Finite Difference Results with Table and Images -->
            <div class="results-with-image">
                <div class="left-content">
                    <div class="performance-table">
                        <table>
                            <tr>
                                <th>Implementation</th>
                                <th>Dx Runtime</th>
                                <th>Dy Runtime</th>
                            </tr>
                            <tr>
                                <td>Naive</td>
                                <td class="time-med">1.47s</td>
                                <td class="time-med">1.60s</td>
                            </tr>
                            <tr>
                                <td>Optimized</td>
                                <td class="time-slow">2.89s</td>
                                <td class="time-slow">2.92s</td>
                            </tr>
                            <tr>
                                <td>Scipy</td>
                                <td class="time-fast">0.01s</td>
                                <td class="time-fast">0.02s</td>
                            </tr>
                        </table>
                    </div>
                    <div class="accuracy-details">
                        <h4>Key Observations</h4>
                        <p>• Small kernels (3×3) show naive faster than optimized due to NumPy overhead.</p>
                        <p>• All implementations produce identical results.</p>
                        <p>• <strong>Dx, Dy outputs:</strong> Can be negative or positive, so black = negative, gray = zero, white = positive.</p>
                        <p>• <strong>Gradient Magnitude:</strong> Always positive, so black = 0 (no edge), white = strong edge.</p>
                    </div>
                </div>
                <div class="right-content">
                    <div class="stacked-images">
                        <figure>
                            <img src="src/img/part1_1_Dx_filter_results.png" alt="Dx Filter" class="clickable">
                            <figcaption>▲ <strong>Dx Result:</strong> Detects vertical edges.</figcaption>
                        </figure>
                        <figure>
                            <img src="src/img/part1_1_Dy_filter_results.png" alt="Dy Filter" class="clickable">
                            <figcaption>▲ <strong>Dy Result:</strong> Detects horizontal edges.</figcaption>
                        </figure>
                        <figure>
                            <img src="src/img/part1_1_Dx_Dy_Gradient Magnitude_results.png" alt="Gradient Magnitude" class="clickable">
                            <figcaption>▲ <strong>Gradient Magnitude:</strong> √(Dx² + Dy²) </figcaption>
                        </figure>
                    </div>
                </div>
            </div>
<!-- 
            <div class="observation-box">
                <h4>Key Observation: Color Representation</h4>
                <p><strong>Dx and Dy outputs:</strong> Can be negative or positive, so zero appears as gray (middle value), with black showing negative gradients and white showing positive gradients.</p>
                <p><strong>Gradient Magnitude:</strong> Always positive (due to squaring), so zero appears as pure black, with brighter values indicating stronger edges regardless of direction.</p>
            </div> -->
        </section>

        <!-- Part 1.2: Edge Detection -->
        <section id="part1-2">
            <h2>Part 1.2: Finite Difference Operator</h2>

            <p>Edge detection via gradient magnitude and thresholding. After learning how to implement convolution from scratch in Part 1.1, we now use scipy for efficiency.</p>

            <!-- Initial Gradient -->
            <h3>1.2.1 Applying Finite Difference to Cameraman</h3>
            <p> Here we apply the same Dx and Dy operators to the classic cameraman.png image and compute the gradient magnitude.</p>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Image Preprocessing
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Important: The cameraman.png downloaded from the project page
# is not a 2D grayscale image, so we need to convert it

if len(cameraman.shape) == 3:
    if cameraman.shape[2] == 4:  # RGBA image
        cameraman = sk.color.rgba2rgb(cameraman)
    cameraman = sk.color.rgb2gray(cameraman)

cameraman = cameraman.astype(np.float64)</code></pre>
                </div>
            </div>

            <div class="image-grid">
                <figure>
                    <img src="src/img/part1_2_partial_derivatives.png" alt="Partial Derivatives" class="clickable">
                    <!-- <figcaption>Partial Derivative in X (Dx), Partial Derivative in Y (Dy), Gradient Magnitude</figcaption> -->
                </figure>
            </div>

            <p>Notice that the contrast is quite poor - the edges appear faint rather than bright white. To address this issue, we need to normalize the gradient magnitude to the range [0,1] and find an optimal threshold to filter out noise -> this is called <b>binary edge detection.</b></p>

            <!-- Normalization -->
            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Gradient Normalization
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Normalize gradient magnitude to [0, 1] range
normalized = gradient_magnitude / np.max(gradient_magnitude)</code></pre>
                </div>
            </div>

            <!-- Threshold Search -->
            <h3>1.2.2 Finding Optimal Threshold</h3>

            <div class="code-block collapsible">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Coarse Search
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Coarse search to find approximate optimal threshold
thresholds = [0.01, 0.05, 0.10, 0.15, 0.20, 0.25]
binary_edges = []

for threshold in thresholds:
    binary_edge = (normalized_gradient_cameraman > threshold).astype(np.float64)
    binary_edges.append(binary_edge)</code></pre>
                </div>
            </div>

            <figure>
                <img src="src/img/part1_2_threshold_comparison_0.01_to_0.25.png" alt="Coarse Search" class="clickable">
                <figcaption>▲ Coarse search: 0.01 (too noisy) → 0.25 (too sparse)</figcaption>
            </figure>

            <div class="code-block collapsible"  style="margin-top: 20px">
                <h4 class="code-toggle" onclick="toggleCode(this)">
                    <span class="toggle-icon">▶</span> Fine-tuned Search
                </h4>
                <div class="code-content" style="display: none;">
                    <pre><code class="language-python"># Fine-tuned search around the promising range
thresholds = [0.19, 0.20, 0.21, 0.22, 0.23]
binary_edges = []

for threshold in thresholds:
    binary_edge = (normalized_gradient_cameraman > threshold).astype(np.float64)
    binary_edges.append(binary_edge)</code></pre>
                </div>
            </div>

            <figure>
                <img src="src/img/part1_2_threshold_comparison_0.19_to_0.23.png" alt="Fine Search" class="clickable">
                <figcaption>▲ Fine-tuning: 0.19-0.23, optimal at 0.21</figcaption>
            </figure>

            <!-- Final Result with Trade-off -->
            <div class="highlight-result-with-tradeoff">
                <img src="src/img/part1_2_edge_threshold_0.21_final.png" alt="Final Edge Detection" class="clickable">
                <div class="right-panel">
                    <div class="result-info">
                        <h4>Optimal: T = 0.21</h4>
                        <p>It's a difficult trade-off: if the threshold is higher, it reduces noise but also blurs the real edges of the cameraman.</p>
                        <p>Since no matter how large the threshold is, the bridge remains as high-frequency as the cameraman, it's impossible to isolate only the cameraman.</p>
                        <p>Hence, this setting is already the best balance between noise suppression and edge preservation.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Part 1.3: DoG Filters -->
        <section id="part1-3">
            <h2>Part 1.3: Derivative of Gaussian (DoG) Filter</h2>

            <p>The edges from Part 1.2 appear noisy. We can improve edge detection by first smoothing the image with a Gaussian filter before computing gradients.</p>
                    <div class="code-block collapsible">
                        <h4 class="code-toggle" onclick="toggleCode(this)">
                            <span class="toggle-icon">▶</span>  Gaussian Kernel Creation
                        </h4>
                        <div class="code-content" style="display: none;">
                            <pre><code class="language-python"># Since a 2D Gaussian is separable:
# G(x,y) = e^(-(x²+y²)/2σ²) = e^(-x²/2σ²) · e^(-y²/2σ²)
#
# So the 2D kernel is just the outer product of the 1D Gaussian with itself.

def create_gaussian_kernel(ksize, sigma):
    gaussian_1d = cv2.getGaussianKernel(ksize, sigma)  # Get 1D Gaussian
    gaussian_2d = gaussian_1d @ gaussian_1d.T           # Outer product
    return gaussian_2d</code></pre>
                        </div>
                    </div>
            <!-- Two-Step Approach -->
            <div style="margin: 30px 0;">
                <h3>1.3.1 Two-Step Approach: Blur Then Gradient</h3>
                    <p>First, we apply a Gaussian filter to smooth the image, then compute gradients:</p>

                    <div class="code-block collapsible">
                        <h4 class="code-toggle" onclick="toggleCode(this)">
                            <span class="toggle-icon">▶</span> Two-Step Implementation
                        </h4>
                        <div class="code-content" style="display: none;">
                            <pre><code class="language-python"># Step 1: Apply Gaussian blur
gaussian_kernel = create_gaussian_kernel(7, 1)
blurred_cameraman = scipy.signal.convolve2d(
    cameraman, gaussian_kernel,
    mode='same', boundary='symm'
)

# Step 2: Apply derivative operators
Dx_blurred = scipy.signal.convolve2d(
    blurred_cameraman, Dx,
    mode='same', boundary='symm'
)
Dy_blurred = scipy.signal.convolve2d(
    blurred_cameraman, Dy,
    mode='same', boundary='symm'
)

# Compute gradient magnitude
gradient_magnitude_blurred = np.sqrt(Dx_blurred**2 + Dy_blurred**2)</code></pre>
                        </div>
                    </div>
<div class="observation-box">
                        <h4>Why Choose kernel_size=7, sigma=1?</h4>
                        <p>A common choice is to choose <strong>k = 6σ + 1</strong></p>
                        <ul>
                            <li>Gaussian kernel contains most (>99%) of the useful weights</li>
                            <li>Kernel size too tiny: cut off the Gaussian</li>
                            <li>Kernel size too large: waste compute since outside the ±3σ weights are very close to 0</li>
                        </ul>
                    </div>
                <figure style="margin-top: 20px;">
                    <img src="src/img/part1_3_two_step_partial_derivatives.png" alt="Two-Step" class="clickable" style="width: 100%;">
                    <figcaption>▲ Two-step: Gaussian smoothing → Gradient</figcaption>
                </figure>
            </div>

            <!-- One-Step Approach -->
            <div style="margin: 30px 0;">
                <h3 style="border-top: 1px solid #e5e5e5; padding-top: 2rem; margin-top: 3rem;">1.3.2 One-Step Approach: DoG Filters</h3>
                    <p>Due to convolution associativity, we can combine the Gaussian and derivative into a single DoG filter:</p>

                <div class="code-block collapsible">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> DoG Filter Implementation
                    </h4>
                    <div class="code-content" style="display: none;">
                        <pre><code class="language-python"># DoG filters: Convolve Gaussian with derivative operators (Dx, Dy) using 'full' mode 
# since we want to keep the entire convolution result, including boundary effects → output is larger than the input.

def create_DoG_kernel(gaussian_kernel, derivative_operators):
    return scipy.signal.convolve2d(gaussian_kernel, derivative_operators, mode='full')

DoG_x = create_DoG_kernel(gaussian_kernel, Dx)
DoG_y = create_DoG_kernel(gaussian_kernel, Dy)
# This is mathematically equivalent but computationally more efficient</code></pre>
                    </div>
                </div>
                <figure style="margin-top: 20px; text-align: center;">
                    <img src="src/img/part1_3_dog_filters_visualization.png" alt="DoG Filter Visualization" class="clickable" style="width: 50%;">
                    <figcaption>▲ DoG Filter Visualization</figcaption>
                </figure>
                <div class="code-block collapsible" style="margin-top: 20px">
                    <h4 class="code-toggle" onclick="toggleCode(this)">
                        <span class="toggle-icon">▶</span> One-Step Implementation: Convolve with DoG
                    </h4>
                <div class="code-content" style="display: none;">
                        <pre><code class="language-python">DoG_x_cameraman = scipy.signal.convolve2d(cameraman, DoG_x, mode='same', boundary='symm')
DoG_y_cameraman = scipy.signal.convolve2d(cameraman, DoG_y, mode='same', boundary='symm')
gradient_magnitude_DoG_cameraman = np.sqrt(DoG_x_cameraman ** 2 + DoG_y_cameraman ** 2)</code></pre>
                    </div>
                </div>
                

                <figure style="margin-top: 20px;">
                    <img src="src/img/part1_3_one_step_partial_derivatives.png" alt="One-Step" class="clickable" style="width: 100%;">
                    <figcaption>▲ One-step DoG approach (identical results)</figcaption>
                </figure>

                <div class="observation-box" style="margin-top: 20px;">
                    <h4>1.3.1 & 1.3.2 Verification: Two-Step vs One-Step</h4>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; align-items: center;">
                        <figure style="margin: 0;">
                            <img src="src/img/part1_3_twostep_vs_dog_comparison.png" alt="Two-Step vs DoG Comparison" class="clickable" style="width: 100%;">
                        </figure>
                        <div>
                            <p><strong>Numerical Verification:</strong></p>
                            <ul style="margin-top: 0.5rem;">
                                <li>Max difference in D<sub>x</sub>: <strong>0.0000000000</strong></li>
                                <li>Max difference in D<sub>y</sub>: <strong>0.0000000000</strong></li>
                                <li>Max difference in gradient magnitude: <strong>0.0000000000</strong></li>
                            </ul>
                            <p style="margin-top: 1rem;">The results are mathematically identical, confirming the associative property of convolution: (G * I) * D = I * (G * D)</p>
                        </div>
                    </div>
                </div>
            </div>

            <h3 style="border-top: 1px solid #e5e5e5; padding-top: 2rem; margin-top: 3rem;">1.3.3 Comparing Edge Detection: With vs Without Blur</h3>
            <p>We can find out that with DoG Filter, the edge is clearer since it <strong>removes high-frequency noise through Gaussian</strong> smoothing while preserving the main edge structures.</p>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                <!-- Left Column: Without Gaussian Blur -->
                <div>
                    <h4 style="text-align: center; margin-bottom: 1rem;">(1.2) Without Gaussian Blur</h4>
                    <figure style="margin: 0 0 1.5rem 0;">
                        <img src="src/img/part1_3_no_blur_partial_derivatives.png" alt="No Blur Derivatives" class="clickable" style="width: 100%;">
                        <figcaption>▲ Partial derivatives (noisy)</figcaption>
                    </figure>
                    <figure style="margin: 0;text-align: center;">
                        <img src="src/img/part1_2_edge_threshold_0.21_final.png" alt="No Blur Edge Detection" class="clickable" style="width: 70%;">
                        <figcaption>▲ Binary edge detection (T=0.21)</figcaption>
                    </figure>
                </div>

                <!-- Right Column: With DoG (Gaussian Blur) -->
                <div>
                    <h4 style="text-align: center; margin-bottom: 1rem;">(1.3) With DoG Filter (Gaussian Blur)</h4>
                    <figure style="margin: 0 0 1.5rem 0;">
                        <img src="src/img/part1_3_one_step_partial_derivatives.png" alt="DoG Derivatives" class="clickable" style="width: 100%;">
                        <figcaption>▲ Partial derivatives (smoothed)</figcaption>
                    </figure>
                    <figure style="margin: 0;text-align: center;">
                        <img src="src/img/part1_3_edge_threshold_0.21_final.png" alt="DoG Edge Detection" class="clickable" style="width: 70%;">
                        <figcaption>▲ Binary edge detection (T=0.21)</figcaption>
                    </figure>
                </div>
            </div>


            <h3>1.3.4 Optimal Threshold with DoG</h3>
            <p>After applying the DoG filter, the high-frequency noise has been removed, which changes the gradient magnitude distribution. Therefore, we need to find a new optimal threshold for edge detection:</p>

            <div class="image-grid">
                <figure>
                    <img src="src/img/part1_3_threshold_comparison_0.01_to_0.50.png" alt="DoG Coarse Search" class="clickable">
                    <figcaption>▲ Coarse search: 0.01 → 0.50</figcaption>
                </figure>
                <figure>
                    <img src="src/img/part1_3_threshold_comparison_0.35_to_0.5.png" alt="DoG Fine Search" class="clickable">
                    <figcaption>▲ Fine-tuning: 0.35 → 0.50</figcaption>
                </figure>
            </div>
            <div class="highlight-result-with-tradeoff" style="grid-template-columns: 1.5fr 1fr;">
                <img src="src/img/part1_3_edge_comparison_threshold_0.41.png" alt="Comparison" class="clickable" style="width: 100%;">
                <div class="right-panel">
                    <div class="result-info">
                        <h4>Optimal: T = 0.41</h4>
                        <p>• DoG allows using higher thresholds (0.41 vs 0.21) while preserving important edges</p>
                        <p>• Without DoG, at threshold 0.41 the edges have already lost half of the details</p>
                    </div>
                </div>
            </div>

        </section>

        <!-- Part 2.1: Sharpening -->
        <section id="part2-1">
            <h2>Part 2.1: Image Sharpening</h2>
            <p>Coming soon...</p>
        </section>

        <!-- Part 2.2: Hybrid Images -->
        <section id="part2-2">
            <h2>Part 2.2: Hybrid Images</h2>
            <p>Coming soon...</p>
        </section>

        <!-- Part 2.3: Stacks -->
        <section id="part2-3">
            <h2>Part 2.3: Gaussian and Laplacian Stacks</h2>
            <p>Coming soon...</p>
        </section>

        <!-- Part 2.4: Blending -->
        <section id="part2-4">
            <h2>Part 2.4: Multi-resolution Blending</h2>
            <p>Coming soon...</p>
        </section>
    </main>

    <!-- Lightbox -->
    <div id="lightbox" class="lightbox">
        <img id="lightbox-img">
        <span class="close">&times;</span>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script>
        // Toggle code blocks
        function toggleCode(element) {
            const codeContent = element.nextElementSibling;
            const toggleIcon = element.querySelector('.toggle-icon');

            if (codeContent.style.display === 'none') {
                codeContent.style.display = 'block';
                toggleIcon.textContent = '▼';
            } else {
                codeContent.style.display = 'none';
                toggleIcon.textContent = '▶';
            }
        }

        // Image lightbox
        document.querySelectorAll('.clickable').forEach(img => {
            img.addEventListener('click', function() {
                const lightbox = document.getElementById('lightbox');
                const lightboxImg = document.getElementById('lightbox-img');
                lightbox.style.display = 'flex';
                lightboxImg.src = this.src;
            });
        });

        document.querySelector('.lightbox .close').addEventListener('click', function() {
            document.getElementById('lightbox').style.display = 'none';
        });

        document.getElementById('lightbox').addEventListener('click', function(e) {
            if (e.target === this) {
                this.style.display = 'none';
            }
        });

        // Smooth scroll
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>